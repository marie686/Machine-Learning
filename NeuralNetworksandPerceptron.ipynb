{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "868c808a-ad29-4d73-be05-18efb33f0773",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils import resample\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4727cb6c-c963-4ba7-adde-a191edfea97f",
   "metadata": {},
   "source": [
    "1.The AUC of the simple Perceptron class I defined with no activation function or hidden layers is 0.5824493173156656. To handle class imbalance I randomly drop some of the majority class which is data labeled nondiabetic. I start with random weight and bias parameters and during training I update them according to the learning rate of .5 I chose for 100 epochs. The predict method makes individual predictions while the prediction method returns a numpy array of predictions to calculate the AUROC.  This means it is not a good predictor because the false positive rate and true positive rate are similar and the model performs basically as well as random guess. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba2b2273-03d9-45cb-b894-20e7cccc0fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5824493173156656\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data=pd.read_csv('/Users/marieqi/Downloads/diabetes.csv')\n",
    "df=data.dropna()\n",
    "scaler=StandardScaler()\n",
    "cols=['BMI', 'MentalHealth','PhysicalHealth', 'AgeBracket', 'EducationBracket', 'IncomeBracket', 'Zodiac']\n",
    "majority_class=df[df['Diabetes']==0]\n",
    "minority_class=df[df['Diabetes']==1]\n",
    "num_samples_to_remove=len(majority_class)-len(minority_class)\n",
    "majority_class_sampled=majority_class.sample(n=num_samples_to_remove,random_state=42)\n",
    "data_balanced=df.drop(majority_class_sampled.index)\n",
    "data_balanced=shuffle(data_balanced,random_state=42)\n",
    "cols=['BMI', 'MentalHealth','PhysicalHealth', 'AgeBracket', 'EducationBracket', 'IncomeBracket', 'Zodiac']\n",
    "data_balanced[cols]=scaler.fit_transform(data_balanced[cols])\n",
    "df[cols]=scaler.fit_transform(df[cols])\n",
    "X=data_balanced.iloc[:, 1:]\n",
    "Y=data_balanced['Diabetes']\n",
    "X_train, X_test, Y_train, Y_test=train_test_split(X,Y,test_size=.2, random_state=42)\n",
    "X_train, X_val, Y_train, y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n",
    "class Perceptron:\n",
    "    def __init__(self):\n",
    "        self.weight=None\n",
    "        self.bias=None\n",
    "    def predict(self,x):\n",
    "        return np.dot(x,self.weight)+self.bias\n",
    "    def train(self,x,y,lr=.5,epochs=100):\n",
    "        self.weight=np.random.randn(x.shape[1])\n",
    "        self.bias=np.random.randn()\n",
    "        for i in range(len(x.iloc[:, 0])):\n",
    "            input_vals=x.iloc[i]  \n",
    "            label=y.iloc[i]              \n",
    "            for _ in range(epochs):\n",
    "                prediction=self.predict(input_vals)\n",
    "                error=label-prediction\n",
    "                self.weight+=lr*error*input_vals\n",
    "                self.bias+=lr*error\n",
    "    def prediction(self,x):\n",
    "        predictions=np.zeros(len(x.iloc[:,0]))\n",
    "        for i in range(len(x.iloc[:, 0])):\n",
    "            predictions[i] = self.predict(x.iloc[i])\n",
    "        return predictions\n",
    "\n",
    "\n",
    "perceptron=Perceptron(tol=1e-3, random_state=0, shuffle=True)\n",
    "perceptron.fit(X_train, Y_train)\n",
    "predictions = perceptron.predict(X_test)\n",
    "fpr,tpr,thresholds=roc_curve(Y_test, predictions)\n",
    "roc_auc=roc_auc_score(Y_test, predictions)\n",
    "print(roc_auc)\n",
    "plt.plot(fpr, tpr, label=f'AUROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Guess')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.show()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a7fd2-c37b-4bdb-9471-ea24f96b7877",
   "metadata": {
    "tags": []
   },
   "source": [
    "2. First I modified he FC2Layer class to have random dropout after each activation to account for class imbalance. Then I made a copy of the class that does not take an activation function meaning there is no nonlinearity introduced in this version. I trained the data with a 2 epochs across all trials for consistency, and changed hidden layers and activation function to see the effect on AUROC. The AUC of the network with 2 hidden layers and Relu activation was 0.7419931317025428 which indiciates moderate performance. The network with 2 hidden layers and sigmoid activation was slightly worse at 0.7132804526236863, which shows relu and sigmoid did not produce a significant difference in accuracy. However sigmoid can cause vanishing gradients with more layers. With 8 hidden layers and sigmoid I get an AUROC of 0.6134508094275479 and we can see the loss values start to stagnate. Using one hidden layer and relu resulted in an AUC of 0.5, which is just random guess. Removing the hidden layers made the AUROC go to .5 which is just random guess, so AUROC score increases with hidden layers. Removing the activation function and using two hidden layers causes the AUROC to improve to  0.7869770661347899, which shows much greater ability to separate classes. Similarly using 1 hidden layer without activation makes AUROC go to 0.7810073551296752, compared to .5 with relu. This implies our data is linearly separable since each output is a linear combination of inputs in a network with no activation function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a88be0a-bd27-4e07-a9f9-9d39bc0d8960",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 56\n",
      "Train Epoch: 0 [0/162355 (0%)]\tLoss: 0.608252\n",
      "Train Epoch: 0 [6400/162355 (4%)]\tLoss: 0.376455\n",
      "Train Epoch: 0 [12800/162355 (8%)]\tLoss: 0.358556\n",
      "Train Epoch: 0 [19200/162355 (12%)]\tLoss: 0.327205\n",
      "Train Epoch: 0 [25600/162355 (16%)]\tLoss: 0.441151\n",
      "Train Epoch: 0 [32000/162355 (20%)]\tLoss: 0.252408\n",
      "Train Epoch: 0 [38400/162355 (24%)]\tLoss: 0.403020\n",
      "Train Epoch: 0 [44800/162355 (28%)]\tLoss: 0.537543\n",
      "Train Epoch: 0 [51200/162355 (32%)]\tLoss: 0.318005\n",
      "Train Epoch: 0 [57600/162355 (35%)]\tLoss: 0.351042\n",
      "Train Epoch: 0 [64000/162355 (39%)]\tLoss: 0.714397\n",
      "Train Epoch: 0 [70400/162355 (43%)]\tLoss: 0.312230\n",
      "Train Epoch: 0 [76800/162355 (47%)]\tLoss: 0.261309\n",
      "Train Epoch: 0 [83200/162355 (51%)]\tLoss: 0.446146\n",
      "Train Epoch: 0 [89600/162355 (55%)]\tLoss: 0.403344\n",
      "Train Epoch: 0 [96000/162355 (59%)]\tLoss: 0.391180\n",
      "Train Epoch: 0 [102400/162355 (63%)]\tLoss: 0.496900\n",
      "Train Epoch: 0 [108800/162355 (67%)]\tLoss: 0.295991\n",
      "Train Epoch: 0 [115200/162355 (71%)]\tLoss: 0.390219\n",
      "Train Epoch: 0 [121600/162355 (75%)]\tLoss: 0.457017\n",
      "Train Epoch: 0 [128000/162355 (79%)]\tLoss: 0.410504\n",
      "Train Epoch: 0 [134400/162355 (83%)]\tLoss: 0.308264\n",
      "Train Epoch: 0 [140800/162355 (87%)]\tLoss: 0.337756\n",
      "Train Epoch: 0 [147200/162355 (91%)]\tLoss: 0.585643\n",
      "Train Epoch: 0 [153600/162355 (95%)]\tLoss: 0.450872\n",
      "Train Epoch: 0 [160000/162355 (99%)]\tLoss: 0.512764\n",
      "Train Epoch: 1 [0/162355 (0%)]\tLoss: 0.348736\n",
      "Train Epoch: 1 [6400/162355 (4%)]\tLoss: 0.365532\n",
      "Train Epoch: 1 [12800/162355 (8%)]\tLoss: 0.419975\n",
      "Train Epoch: 1 [19200/162355 (12%)]\tLoss: 0.423685\n",
      "Train Epoch: 1 [25600/162355 (16%)]\tLoss: 0.364217\n",
      "Train Epoch: 1 [32000/162355 (20%)]\tLoss: 0.426577\n",
      "Train Epoch: 1 [38400/162355 (24%)]\tLoss: 0.253793\n",
      "Train Epoch: 1 [44800/162355 (28%)]\tLoss: 0.309745\n",
      "Train Epoch: 1 [51200/162355 (32%)]\tLoss: 0.507452\n",
      "Train Epoch: 1 [57600/162355 (35%)]\tLoss: 0.330863\n",
      "Train Epoch: 1 [64000/162355 (39%)]\tLoss: 0.356385\n",
      "Train Epoch: 1 [70400/162355 (43%)]\tLoss: 0.428202\n",
      "Train Epoch: 1 [76800/162355 (47%)]\tLoss: 0.360972\n",
      "Train Epoch: 1 [83200/162355 (51%)]\tLoss: 0.428694\n",
      "Train Epoch: 1 [89600/162355 (55%)]\tLoss: 0.339514\n",
      "Train Epoch: 1 [96000/162355 (59%)]\tLoss: 0.476962\n",
      "Train Epoch: 1 [102400/162355 (63%)]\tLoss: 0.365168\n",
      "Train Epoch: 1 [108800/162355 (67%)]\tLoss: 0.488988\n",
      "Train Epoch: 1 [115200/162355 (71%)]\tLoss: 0.363554\n",
      "Train Epoch: 1 [121600/162355 (75%)]\tLoss: 0.418504\n",
      "Train Epoch: 1 [128000/162355 (79%)]\tLoss: 0.504492\n",
      "Train Epoch: 1 [134400/162355 (83%)]\tLoss: 0.552034\n",
      "Train Epoch: 1 [140800/162355 (87%)]\tLoss: 0.243642\n",
      "Train Epoch: 1 [147200/162355 (91%)]\tLoss: 0.367674\n",
      "Train Epoch: 1 [153600/162355 (95%)]\tLoss: 0.548347\n",
      "Train Epoch: 1 [160000/162355 (99%)]\tLoss: 0.429680\n",
      "ROC AUC Score 2 hidden layers relu: 0.7419931317025428\n",
      "Number of parameters: 2\n",
      "Train Epoch: 0 [0/162355 (0%)]\tLoss: 0.693147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marieqi/anaconda3/lib/python3.11/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [6400/162355 (4%)]\tLoss: 0.550219\n",
      "Train Epoch: 0 [12800/162355 (8%)]\tLoss: 0.442375\n",
      "Train Epoch: 0 [19200/162355 (12%)]\tLoss: 0.483035\n",
      "Train Epoch: 0 [25600/162355 (16%)]\tLoss: 0.354554\n",
      "Train Epoch: 0 [32000/162355 (20%)]\tLoss: 0.352759\n",
      "Train Epoch: 0 [38400/162355 (24%)]\tLoss: 0.488867\n",
      "Train Epoch: 0 [44800/162355 (28%)]\tLoss: 0.378346\n",
      "Train Epoch: 0 [51200/162355 (32%)]\tLoss: 0.239219\n",
      "Train Epoch: 0 [57600/162355 (35%)]\tLoss: 0.434157\n",
      "Train Epoch: 0 [64000/162355 (39%)]\tLoss: 0.290893\n",
      "Train Epoch: 0 [70400/162355 (43%)]\tLoss: 0.463617\n",
      "Train Epoch: 0 [76800/162355 (47%)]\tLoss: 0.406128\n",
      "Train Epoch: 0 [83200/162355 (51%)]\tLoss: 0.320070\n",
      "Train Epoch: 0 [89600/162355 (55%)]\tLoss: 0.462992\n",
      "Train Epoch: 0 [96000/162355 (59%)]\tLoss: 0.320083\n",
      "Train Epoch: 0 [102400/162355 (63%)]\tLoss: 0.547848\n",
      "Train Epoch: 0 [108800/162355 (67%)]\tLoss: 0.520589\n",
      "Train Epoch: 0 [115200/162355 (71%)]\tLoss: 0.463184\n",
      "Train Epoch: 0 [121600/162355 (75%)]\tLoss: 0.491213\n",
      "Train Epoch: 0 [128000/162355 (79%)]\tLoss: 0.434589\n",
      "Train Epoch: 0 [134400/162355 (83%)]\tLoss: 0.349694\n",
      "Train Epoch: 0 [140800/162355 (87%)]\tLoss: 0.573126\n",
      "Train Epoch: 0 [147200/162355 (91%)]\tLoss: 0.517748\n",
      "Train Epoch: 0 [153600/162355 (95%)]\tLoss: 0.490367\n",
      "Train Epoch: 0 [160000/162355 (99%)]\tLoss: 0.377758\n",
      "Train Epoch: 1 [0/162355 (0%)]\tLoss: 0.321164\n",
      "Train Epoch: 1 [6400/162355 (4%)]\tLoss: 0.263958\n",
      "Train Epoch: 1 [12800/162355 (8%)]\tLoss: 0.406097\n",
      "Train Epoch: 1 [19200/162355 (12%)]\tLoss: 0.406112\n",
      "Train Epoch: 1 [25600/162355 (16%)]\tLoss: 0.264653\n",
      "Train Epoch: 1 [32000/162355 (20%)]\tLoss: 0.320641\n",
      "Train Epoch: 1 [38400/162355 (24%)]\tLoss: 0.291926\n",
      "Train Epoch: 1 [44800/162355 (28%)]\tLoss: 0.463831\n",
      "Train Epoch: 1 [51200/162355 (32%)]\tLoss: 0.463038\n",
      "Train Epoch: 1 [57600/162355 (35%)]\tLoss: 0.463372\n",
      "Train Epoch: 1 [64000/162355 (39%)]\tLoss: 0.463801\n",
      "Train Epoch: 1 [70400/162355 (43%)]\tLoss: 0.377273\n",
      "Train Epoch: 1 [76800/162355 (47%)]\tLoss: 0.694099\n",
      "Train Epoch: 1 [83200/162355 (51%)]\tLoss: 0.320705\n",
      "Train Epoch: 1 [89600/162355 (55%)]\tLoss: 0.321090\n",
      "Train Epoch: 1 [96000/162355 (59%)]\tLoss: 0.548534\n",
      "Train Epoch: 1 [102400/162355 (63%)]\tLoss: 0.406098\n",
      "Train Epoch: 1 [108800/162355 (67%)]\tLoss: 0.462925\n",
      "Train Epoch: 1 [115200/162355 (71%)]\tLoss: 0.463273\n",
      "Train Epoch: 1 [121600/162355 (75%)]\tLoss: 0.377486\n",
      "Train Epoch: 1 [128000/162355 (79%)]\tLoss: 0.434244\n",
      "Train Epoch: 1 [134400/162355 (83%)]\tLoss: 0.545947\n",
      "Train Epoch: 1 [140800/162355 (87%)]\tLoss: 0.547465\n",
      "Train Epoch: 1 [147200/162355 (91%)]\tLoss: 0.349939\n",
      "Train Epoch: 1 [153600/162355 (95%)]\tLoss: 0.349430\n",
      "Train Epoch: 1 [160000/162355 (99%)]\tLoss: 0.322570\n",
      "ROC AUC Score 0 hidden layers relu: 0.5\n",
      "Number of parameters: 56\n",
      "Train Epoch: 0 [0/162355 (0%)]\tLoss: 0.617346\n",
      "Train Epoch: 0 [6400/162355 (4%)]\tLoss: 0.548892\n",
      "Train Epoch: 0 [12800/162355 (8%)]\tLoss: 0.621454\n",
      "Train Epoch: 0 [19200/162355 (12%)]\tLoss: 0.314412\n",
      "Train Epoch: 0 [25600/162355 (16%)]\tLoss: 0.381398\n",
      "Train Epoch: 0 [32000/162355 (20%)]\tLoss: 0.387071\n",
      "Train Epoch: 0 [38400/162355 (24%)]\tLoss: 0.605786\n",
      "Train Epoch: 0 [44800/162355 (28%)]\tLoss: 0.487112\n",
      "Train Epoch: 0 [51200/162355 (32%)]\tLoss: 0.401347\n",
      "Train Epoch: 0 [57600/162355 (35%)]\tLoss: 0.278387\n",
      "Train Epoch: 0 [64000/162355 (39%)]\tLoss: 0.528341\n",
      "Train Epoch: 0 [70400/162355 (43%)]\tLoss: 0.446247\n",
      "Train Epoch: 0 [76800/162355 (47%)]\tLoss: 0.447812\n",
      "Train Epoch: 0 [83200/162355 (51%)]\tLoss: 0.424581\n",
      "Train Epoch: 0 [89600/162355 (55%)]\tLoss: 0.457285\n",
      "Train Epoch: 0 [96000/162355 (59%)]\tLoss: 0.351909\n",
      "Train Epoch: 0 [102400/162355 (63%)]\tLoss: 0.322546\n",
      "Train Epoch: 0 [108800/162355 (67%)]\tLoss: 0.519870\n",
      "Train Epoch: 0 [115200/162355 (71%)]\tLoss: 0.402481\n",
      "Train Epoch: 0 [121600/162355 (75%)]\tLoss: 0.465853\n",
      "Train Epoch: 0 [128000/162355 (79%)]\tLoss: 0.434027\n",
      "Train Epoch: 0 [134400/162355 (83%)]\tLoss: 0.489900\n",
      "Train Epoch: 0 [140800/162355 (87%)]\tLoss: 0.413282\n",
      "Train Epoch: 0 [147200/162355 (91%)]\tLoss: 0.260123\n",
      "Train Epoch: 0 [153600/162355 (95%)]\tLoss: 0.424111\n",
      "Train Epoch: 0 [160000/162355 (99%)]\tLoss: 0.378161\n",
      "Train Epoch: 1 [0/162355 (0%)]\tLoss: 0.288619\n",
      "Train Epoch: 1 [6400/162355 (4%)]\tLoss: 0.378288\n",
      "Train Epoch: 1 [12800/162355 (8%)]\tLoss: 0.286224\n",
      "Train Epoch: 1 [19200/162355 (12%)]\tLoss: 0.329523\n",
      "Train Epoch: 1 [25600/162355 (16%)]\tLoss: 0.513530\n",
      "Train Epoch: 1 [32000/162355 (20%)]\tLoss: 0.426314\n",
      "Train Epoch: 1 [38400/162355 (24%)]\tLoss: 0.488040\n",
      "Train Epoch: 1 [44800/162355 (28%)]\tLoss: 0.453015\n",
      "Train Epoch: 1 [51200/162355 (32%)]\tLoss: 0.316030\n",
      "Train Epoch: 1 [57600/162355 (35%)]\tLoss: 0.259787\n",
      "Train Epoch: 1 [64000/162355 (39%)]\tLoss: 0.513702\n",
      "Train Epoch: 1 [70400/162355 (43%)]\tLoss: 0.403049\n",
      "Train Epoch: 1 [76800/162355 (47%)]\tLoss: 0.344341\n",
      "Train Epoch: 1 [83200/162355 (51%)]\tLoss: 0.401872\n",
      "Train Epoch: 1 [89600/162355 (55%)]\tLoss: 0.488309\n",
      "Train Epoch: 1 [96000/162355 (59%)]\tLoss: 0.322665\n",
      "Train Epoch: 1 [102400/162355 (63%)]\tLoss: 0.295191\n",
      "Train Epoch: 1 [108800/162355 (67%)]\tLoss: 0.519623\n",
      "Train Epoch: 1 [115200/162355 (71%)]\tLoss: 0.464833\n",
      "Train Epoch: 1 [121600/162355 (75%)]\tLoss: 0.463983\n",
      "Train Epoch: 1 [128000/162355 (79%)]\tLoss: 0.491490\n",
      "Train Epoch: 1 [134400/162355 (83%)]\tLoss: 0.370323\n",
      "Train Epoch: 1 [140800/162355 (87%)]\tLoss: 0.346944\n",
      "Train Epoch: 1 [147200/162355 (91%)]\tLoss: 0.295616\n",
      "Train Epoch: 1 [153600/162355 (95%)]\tLoss: 0.259613\n",
      "Train Epoch: 1 [160000/162355 (99%)]\tLoss: 0.404326\n",
      "ROC AUC Score 2 hidden layers sigmoid: 0.7132804526236863\n",
      "Number of parameters: 56\n",
      "Train Epoch: 0 [0/162355 (0%)]\tLoss: 1.012051\n",
      "Train Epoch: 0 [6400/162355 (4%)]\tLoss: 0.605068\n",
      "Train Epoch: 0 [12800/162355 (8%)]\tLoss: 0.587235\n",
      "Train Epoch: 0 [19200/162355 (12%)]\tLoss: 0.416470\n",
      "Train Epoch: 0 [25600/162355 (16%)]\tLoss: 0.380036\n",
      "Train Epoch: 0 [32000/162355 (20%)]\tLoss: 0.399787\n",
      "Train Epoch: 0 [38400/162355 (24%)]\tLoss: 0.352909\n",
      "Train Epoch: 0 [44800/162355 (28%)]\tLoss: 0.381023\n",
      "Train Epoch: 0 [51200/162355 (32%)]\tLoss: 0.375768\n",
      "Train Epoch: 0 [57600/162355 (35%)]\tLoss: 0.397184\n",
      "Train Epoch: 0 [64000/162355 (39%)]\tLoss: 0.294050\n",
      "Train Epoch: 0 [70400/162355 (43%)]\tLoss: 0.375965\n",
      "Train Epoch: 0 [76800/162355 (47%)]\tLoss: 0.368656\n",
      "Train Epoch: 0 [83200/162355 (51%)]\tLoss: 0.575753\n",
      "Train Epoch: 0 [89600/162355 (55%)]\tLoss: 0.419297\n",
      "Train Epoch: 0 [96000/162355 (59%)]\tLoss: 0.399316\n",
      "Train Epoch: 0 [102400/162355 (63%)]\tLoss: 0.430775\n",
      "Train Epoch: 0 [108800/162355 (67%)]\tLoss: 0.404995\n",
      "Train Epoch: 0 [115200/162355 (71%)]\tLoss: 0.416048\n",
      "Train Epoch: 0 [121600/162355 (75%)]\tLoss: 0.348483\n",
      "Train Epoch: 0 [128000/162355 (79%)]\tLoss: 0.425292\n",
      "Train Epoch: 0 [134400/162355 (83%)]\tLoss: 0.441025\n",
      "Train Epoch: 0 [140800/162355 (87%)]\tLoss: 0.344734\n",
      "Train Epoch: 0 [147200/162355 (91%)]\tLoss: 0.447676\n",
      "Train Epoch: 0 [153600/162355 (95%)]\tLoss: 0.344744\n",
      "Train Epoch: 0 [160000/162355 (99%)]\tLoss: 0.501637\n",
      "Train Epoch: 1 [0/162355 (0%)]\tLoss: 0.396284\n",
      "Train Epoch: 1 [6400/162355 (4%)]\tLoss: 0.391459\n",
      "Train Epoch: 1 [12800/162355 (8%)]\tLoss: 0.340816\n",
      "Train Epoch: 1 [19200/162355 (12%)]\tLoss: 0.281160\n",
      "Train Epoch: 1 [25600/162355 (16%)]\tLoss: 0.368340\n",
      "Train Epoch: 1 [32000/162355 (20%)]\tLoss: 0.436403\n",
      "Train Epoch: 1 [38400/162355 (24%)]\tLoss: 0.422830\n",
      "Train Epoch: 1 [44800/162355 (28%)]\tLoss: 0.476548\n",
      "Train Epoch: 1 [51200/162355 (32%)]\tLoss: 0.343331\n",
      "Train Epoch: 1 [57600/162355 (35%)]\tLoss: 0.396547\n",
      "Train Epoch: 1 [64000/162355 (39%)]\tLoss: 0.374387\n",
      "Train Epoch: 1 [70400/162355 (43%)]\tLoss: 0.324831\n",
      "Train Epoch: 1 [76800/162355 (47%)]\tLoss: 0.397098\n",
      "Train Epoch: 1 [83200/162355 (51%)]\tLoss: 0.452996\n",
      "Train Epoch: 1 [89600/162355 (55%)]\tLoss: 0.525369\n",
      "Train Epoch: 1 [96000/162355 (59%)]\tLoss: 0.506455\n",
      "Train Epoch: 1 [102400/162355 (63%)]\tLoss: 0.292446\n",
      "Train Epoch: 1 [108800/162355 (67%)]\tLoss: 0.447168\n",
      "Train Epoch: 1 [115200/162355 (71%)]\tLoss: 0.441065\n",
      "Train Epoch: 1 [121600/162355 (75%)]\tLoss: 0.407699\n",
      "Train Epoch: 1 [128000/162355 (79%)]\tLoss: 0.369028\n",
      "Train Epoch: 1 [134400/162355 (83%)]\tLoss: 0.556739\n",
      "Train Epoch: 1 [140800/162355 (87%)]\tLoss: 0.480369\n",
      "Train Epoch: 1 [147200/162355 (91%)]\tLoss: 0.405407\n",
      "Train Epoch: 1 [153600/162355 (95%)]\tLoss: 0.320008\n",
      "Train Epoch: 1 [160000/162355 (99%)]\tLoss: 0.261172\n",
      "ROC AUC Score 2 hidden layers no activation: 0.7869770661347899\n",
      "Number of parameters: 28\n",
      "Train Epoch: 0 [0/162355 (0%)]\tLoss: 0.398926\n",
      "Train Epoch: 0 [6400/162355 (4%)]\tLoss: 0.384883\n",
      "Train Epoch: 0 [12800/162355 (8%)]\tLoss: 0.433414\n",
      "Train Epoch: 0 [19200/162355 (12%)]\tLoss: 0.325484\n",
      "Train Epoch: 0 [25600/162355 (16%)]\tLoss: 0.406231\n",
      "Train Epoch: 0 [32000/162355 (20%)]\tLoss: 0.265281\n",
      "Train Epoch: 0 [38400/162355 (24%)]\tLoss: 0.406112\n",
      "Train Epoch: 0 [44800/162355 (28%)]\tLoss: 0.663321\n",
      "Train Epoch: 0 [51200/162355 (32%)]\tLoss: 0.265467\n",
      "Train Epoch: 0 [57600/162355 (35%)]\tLoss: 0.349319\n",
      "Train Epoch: 0 [64000/162355 (39%)]\tLoss: 0.490858\n",
      "Train Epoch: 0 [70400/162355 (43%)]\tLoss: 0.377878\n",
      "Train Epoch: 0 [76800/162355 (47%)]\tLoss: 0.377725\n",
      "Train Epoch: 0 [83200/162355 (51%)]\tLoss: 0.378033\n",
      "Train Epoch: 0 [89600/162355 (55%)]\tLoss: 0.349764\n",
      "Train Epoch: 0 [96000/162355 (59%)]\tLoss: 0.462539\n",
      "Train Epoch: 0 [102400/162355 (63%)]\tLoss: 0.377958\n",
      "Train Epoch: 0 [108800/162355 (67%)]\tLoss: 0.406100\n",
      "Train Epoch: 0 [115200/162355 (71%)]\tLoss: 0.293281\n",
      "Train Epoch: 0 [121600/162355 (75%)]\tLoss: 0.377586\n",
      "Train Epoch: 0 [128000/162355 (79%)]\tLoss: 0.265039\n",
      "Train Epoch: 0 [134400/162355 (83%)]\tLoss: 0.463452\n",
      "Train Epoch: 0 [140800/162355 (87%)]\tLoss: 0.518120\n",
      "Train Epoch: 0 [147200/162355 (91%)]\tLoss: 0.519877\n",
      "Train Epoch: 0 [153600/162355 (95%)]\tLoss: 0.546936\n",
      "Train Epoch: 0 [160000/162355 (99%)]\tLoss: 0.292601\n",
      "Train Epoch: 1 [0/162355 (0%)]\tLoss: 0.320626\n",
      "Train Epoch: 1 [6400/162355 (4%)]\tLoss: 0.291984\n",
      "Train Epoch: 1 [12800/162355 (8%)]\tLoss: 0.434479\n",
      "Train Epoch: 1 [19200/162355 (12%)]\tLoss: 0.378179\n",
      "Train Epoch: 1 [25600/162355 (16%)]\tLoss: 0.406097\n",
      "Train Epoch: 1 [32000/162355 (20%)]\tLoss: 0.321242\n",
      "Train Epoch: 1 [38400/162355 (24%)]\tLoss: 0.349652\n",
      "Train Epoch: 1 [44800/162355 (28%)]\tLoss: 0.490867\n",
      "Train Epoch: 1 [51200/162355 (32%)]\tLoss: 0.434385\n",
      "Train Epoch: 1 [57600/162355 (35%)]\tLoss: 0.377617\n",
      "Train Epoch: 1 [64000/162355 (39%)]\tLoss: 0.434799\n",
      "Train Epoch: 1 [70400/162355 (43%)]\tLoss: 0.519505\n",
      "Train Epoch: 1 [76800/162355 (47%)]\tLoss: 0.434409\n",
      "Train Epoch: 1 [83200/162355 (51%)]\tLoss: 0.349699\n",
      "Train Epoch: 1 [89600/162355 (55%)]\tLoss: 0.377578\n",
      "Train Epoch: 1 [96000/162355 (59%)]\tLoss: 0.491452\n",
      "Train Epoch: 1 [102400/162355 (63%)]\tLoss: 0.632778\n",
      "Train Epoch: 1 [108800/162355 (67%)]\tLoss: 0.406102\n",
      "Train Epoch: 1 [115200/162355 (71%)]\tLoss: 0.547209\n",
      "Train Epoch: 1 [121600/162355 (75%)]\tLoss: 0.434686\n",
      "Train Epoch: 1 [128000/162355 (79%)]\tLoss: 0.377825\n",
      "Train Epoch: 1 [134400/162355 (83%)]\tLoss: 0.348730\n",
      "Train Epoch: 1 [140800/162355 (87%)]\tLoss: 0.349140\n",
      "Train Epoch: 1 [147200/162355 (91%)]\tLoss: 0.406112\n",
      "Train Epoch: 1 [153600/162355 (95%)]\tLoss: 0.406141\n",
      "Train Epoch: 1 [160000/162355 (99%)]\tLoss: 0.406146\n",
      "ROC AUC Score 1 hidden layer relu: 0.5\n",
      "Number of parameters: 28\n",
      "Train Epoch: 0 [0/162355 (0%)]\tLoss: 2.160082\n",
      "Train Epoch: 0 [6400/162355 (4%)]\tLoss: 0.638544\n",
      "Train Epoch: 0 [12800/162355 (8%)]\tLoss: 0.506357\n",
      "Train Epoch: 0 [19200/162355 (12%)]\tLoss: 0.375959\n",
      "Train Epoch: 0 [25600/162355 (16%)]\tLoss: 0.388404\n",
      "Train Epoch: 0 [32000/162355 (20%)]\tLoss: 0.501676\n",
      "Train Epoch: 0 [38400/162355 (24%)]\tLoss: 0.416174\n",
      "Train Epoch: 0 [44800/162355 (28%)]\tLoss: 0.454330\n",
      "Train Epoch: 0 [51200/162355 (32%)]\tLoss: 0.356999\n",
      "Train Epoch: 0 [57600/162355 (35%)]\tLoss: 0.380298\n",
      "Train Epoch: 0 [64000/162355 (39%)]\tLoss: 0.451384\n",
      "Train Epoch: 0 [70400/162355 (43%)]\tLoss: 0.333763\n",
      "Train Epoch: 0 [76800/162355 (47%)]\tLoss: 0.431626\n",
      "Train Epoch: 0 [83200/162355 (51%)]\tLoss: 0.469770\n",
      "Train Epoch: 0 [89600/162355 (55%)]\tLoss: 0.592884\n",
      "Train Epoch: 0 [96000/162355 (59%)]\tLoss: 0.622461\n",
      "Train Epoch: 0 [102400/162355 (63%)]\tLoss: 0.486592\n",
      "Train Epoch: 0 [108800/162355 (67%)]\tLoss: 0.315610\n",
      "Train Epoch: 0 [115200/162355 (71%)]\tLoss: 0.457230\n",
      "Train Epoch: 0 [121600/162355 (75%)]\tLoss: 0.341171\n",
      "Train Epoch: 0 [128000/162355 (79%)]\tLoss: 0.327083\n",
      "Train Epoch: 0 [134400/162355 (83%)]\tLoss: 0.491843\n",
      "Train Epoch: 0 [140800/162355 (87%)]\tLoss: 0.456225\n",
      "Train Epoch: 0 [147200/162355 (91%)]\tLoss: 0.359090\n",
      "Train Epoch: 0 [153600/162355 (95%)]\tLoss: 0.294355\n",
      "Train Epoch: 0 [160000/162355 (99%)]\tLoss: 0.466388\n",
      "Train Epoch: 1 [0/162355 (0%)]\tLoss: 0.448784\n",
      "Train Epoch: 1 [6400/162355 (4%)]\tLoss: 0.464295\n",
      "Train Epoch: 1 [12800/162355 (8%)]\tLoss: 0.375794\n",
      "Train Epoch: 1 [19200/162355 (12%)]\tLoss: 0.410874\n",
      "Train Epoch: 1 [25600/162355 (16%)]\tLoss: 0.451622\n",
      "Train Epoch: 1 [32000/162355 (20%)]\tLoss: 0.370533\n",
      "Train Epoch: 1 [38400/162355 (24%)]\tLoss: 0.323600\n",
      "Train Epoch: 1 [44800/162355 (28%)]\tLoss: 0.345346\n",
      "Train Epoch: 1 [51200/162355 (32%)]\tLoss: 0.488423\n",
      "Train Epoch: 1 [57600/162355 (35%)]\tLoss: 0.408322\n",
      "Train Epoch: 1 [64000/162355 (39%)]\tLoss: 0.412408\n",
      "Train Epoch: 1 [70400/162355 (43%)]\tLoss: 0.192246\n",
      "Train Epoch: 1 [76800/162355 (47%)]\tLoss: 0.365862\n",
      "Train Epoch: 1 [83200/162355 (51%)]\tLoss: 0.363812\n",
      "Train Epoch: 1 [89600/162355 (55%)]\tLoss: 0.492012\n",
      "Train Epoch: 1 [96000/162355 (59%)]\tLoss: 0.375529\n",
      "Train Epoch: 1 [102400/162355 (63%)]\tLoss: 0.285161\n",
      "Train Epoch: 1 [108800/162355 (67%)]\tLoss: 0.451712\n",
      "Train Epoch: 1 [115200/162355 (71%)]\tLoss: 0.539665\n",
      "Train Epoch: 1 [121600/162355 (75%)]\tLoss: 0.426242\n",
      "Train Epoch: 1 [128000/162355 (79%)]\tLoss: 0.469184\n",
      "Train Epoch: 1 [134400/162355 (83%)]\tLoss: 0.488097\n",
      "Train Epoch: 1 [140800/162355 (87%)]\tLoss: 0.390251\n",
      "Train Epoch: 1 [147200/162355 (91%)]\tLoss: 0.367469\n",
      "Train Epoch: 1 [153600/162355 (95%)]\tLoss: 0.363287\n",
      "Train Epoch: 1 [160000/162355 (99%)]\tLoss: 0.373204\n",
      "ROC AUC Score 1 hidden layer no activation: 0.7810073551296752\n"
     ]
    }
   ],
   "source": [
    "#modified code from lab6\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "class FC2Layer(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden, output_size, dropout_prob, activation):\n",
    "        super(FC2Layer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.dropout_prob = dropout_prob\n",
    "        if activation =='relu':\n",
    "            self.activation = nn.ReLU()  \n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        else :\n",
    "            self.activation = nn.ReLU()\n",
    "            print(\"Activation function not implemented, using default ReLU\")\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, n_hidden), \n",
    "            self.activation,\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(n_hidden, n_hidden), \n",
    "            self.activation,\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(n_hidden, output_size), \n",
    "            nn.LogSoftmax(dim=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)      \n",
    "        return self.network(x)\n",
    "class FC2Layer2(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden, output_size, dropout_prob):\n",
    "        super(FC2Layer2, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, n_hidden), \n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(n_hidden, n_hidden), \n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(n_hidden, output_size), \n",
    "            nn.LogSoftmax(dim=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)      \n",
    "        return self.network(x)\n",
    "X=df.iloc[:, 1:]\n",
    "Y=df['Diabetes']\n",
    "X_train, X_test, Y_train, Y_test=train_test_split(X,Y,test_size=.2, random_state=42)\n",
    "X_train, X_val, Y_train, y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n",
    "def train(epoch, model, optimizer, perm=torch.arange(0, 21).long(), verbose=False):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        losses = []\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(-1, 21)\n",
    "            data = data[:, perm]\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0:\n",
    "                losses.append(loss.detach())\n",
    "                if verbose :\n",
    "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                        100. * batch_idx / len(train_loader), loss.item()))\n",
    "                    \n",
    "        return losses\n",
    "def test(model, perm=torch.arange(0, 21).long(), verbose=False):\n",
    "    model.eval()\n",
    "    accuracy_list = []\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(-1, 21)\n",
    "            data = data[:, perm]\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.sum().item()                                                      \n",
    "            pred = output.data.max(1, keepdim=True)[1] \n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        accuracy_list.append(accuracy) \n",
    "        if verbose :\n",
    "            print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                test_loss, correct, len(test_loader.dataset),\n",
    "                accuracy))\n",
    "    return test_loss       \n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X,y):\n",
    "        self.X_data = X\n",
    "        self.y_data = y  \n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.Tensor(self.X_data.iloc[idx].values)\n",
    "        y = torch.LongTensor([self.y_data.iloc[idx]])\n",
    "        return X, y.squeeze() \n",
    "def get_n_params(model):\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.nelement()\n",
    "    return np\n",
    "def calculate_auc(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true=[]\n",
    "    y_scores=[]\n",
    "    with torch.no_grad():\n",
    "        for data,target in test_loader:\n",
    "            data,target = data.to(device),target.to(device)\n",
    "            output=model(data)\n",
    "            y_true.extend(target.cpu().numpy())\n",
    "            y_scores.extend(output[:, 1].cpu().numpy())  \n",
    "    return roc_auc_score(y_true, y_scores)\n",
    "train_dataset = CustomDataset(X_train,Y_train)\n",
    "test_dataset = CustomDataset(X_test,Y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=True)\n",
    "input_size=21\n",
    "output_size=2\n",
    "n_hidden=2 \n",
    "model_fnn=FC2Layer(input_size, n_hidden, output_size, .5, 'relu')\n",
    "model_fnn=model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "for epoch in range(0, 2):\n",
    "    train(epoch, model_fnn, optimizer, verbose=True)\n",
    "auc_score = calculate_auc(model_fnn, test_loader)\n",
    "print(\"ROC AUC Score 2 hidden layers relu:\", auc_score)\n",
    "input_size=21\n",
    "output_size=2\n",
    "n_hidden=0 \n",
    "model_fnn=FC2Layer(input_size, n_hidden, output_size, .5, 'relu')\n",
    "model_fnn=model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "for epoch in range(0, 2):\n",
    "    train(epoch, model_fnn, optimizer, verbose=True)\n",
    "auc_score = calculate_auc(model_fnn, test_loader)\n",
    "print(\"ROC AUC Score 0 hidden layers relu:\", auc_score)\n",
    "input_size=21\n",
    "output_size=2\n",
    "n_hidden=2 \n",
    "model_fnn=FC2Layer(input_size, n_hidden, output_size, .5, 'sigmoid')\n",
    "model_fnn=model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "for epoch in range(0, 2):\n",
    "    train(epoch, model_fnn, optimizer, verbose=True)\n",
    "auc_score = calculate_auc(model_fnn, test_loader)\n",
    "print(\"ROC AUC Score 2 hidden layers sigmoid:\", auc_score)\n",
    "input_size=21\n",
    "output_size=2\n",
    "n_hidden=2 \n",
    "model_fnn=FC2Layer2(input_size, n_hidden, output_size, .5)\n",
    "model_fnn=model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "for epoch in range(0, 2):\n",
    "    train(epoch, model_fnn, optimizer, verbose=True)\n",
    "auc_score = calculate_auc(model_fnn, test_loader)\n",
    "print(\"ROC AUC Score 2 hidden layers no activation:\", auc_score)\n",
    "input_size=21\n",
    "output_size=2\n",
    "n_hidden=1 \n",
    "model_fnn=FC2Layer(input_size, n_hidden, output_size, .5, 'relu')\n",
    "model_fnn=model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "for epoch in range(0, 2):\n",
    "    train(epoch, model_fnn, optimizer, verbose=True)\n",
    "auc_score = calculate_auc(model_fnn, test_loader)\n",
    "print(\"ROC AUC Score 1 hidden layer relu:\", auc_score)\n",
    "input_size=21\n",
    "output_size=2\n",
    "n_hidden=1 \n",
    "model_fnn=FC2Layer2(input_size, n_hidden, output_size, .5)\n",
    "model_fnn=model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "for epoch in range(0, 2):\n",
    "    train(epoch, model_fnn, optimizer, verbose=True)\n",
    "auc_score = calculate_auc(model_fnn, test_loader)\n",
    "print(\"ROC AUC Score 1 hidden layer no activation:\", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc95fd41-5969-4060-85aa-d07183ad612e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/162355 (0%)]\tLoss: 1.005408\n",
      "Train Epoch: 0 [6400/162355 (4%)]\tLoss: 0.397755\n",
      "Train Epoch: 0 [12800/162355 (8%)]\tLoss: 0.442692\n",
      "Train Epoch: 0 [19200/162355 (12%)]\tLoss: 0.520413\n",
      "Train Epoch: 0 [25600/162355 (16%)]\tLoss: 0.293806\n",
      "Train Epoch: 0 [32000/162355 (20%)]\tLoss: 0.240977\n",
      "Train Epoch: 0 [38400/162355 (24%)]\tLoss: 0.425977\n",
      "Train Epoch: 0 [44800/162355 (28%)]\tLoss: 0.359309\n",
      "Train Epoch: 0 [51200/162355 (32%)]\tLoss: 0.469069\n",
      "Train Epoch: 0 [57600/162355 (35%)]\tLoss: 0.479784\n",
      "Train Epoch: 0 [64000/162355 (39%)]\tLoss: 0.423252\n",
      "Train Epoch: 0 [70400/162355 (43%)]\tLoss: 0.352696\n",
      "Train Epoch: 0 [76800/162355 (47%)]\tLoss: 0.319812\n",
      "Train Epoch: 0 [83200/162355 (51%)]\tLoss: 0.312359\n",
      "Train Epoch: 0 [89600/162355 (55%)]\tLoss: 0.408188\n",
      "Train Epoch: 0 [96000/162355 (59%)]\tLoss: 0.362331\n",
      "Train Epoch: 0 [102400/162355 (63%)]\tLoss: 0.509366\n",
      "Train Epoch: 0 [108800/162355 (67%)]\tLoss: 0.439966\n",
      "Train Epoch: 0 [115200/162355 (71%)]\tLoss: 0.420936\n",
      "Train Epoch: 0 [121600/162355 (75%)]\tLoss: 0.427654\n",
      "Train Epoch: 0 [128000/162355 (79%)]\tLoss: 0.480153\n",
      "Train Epoch: 0 [134400/162355 (83%)]\tLoss: 0.462308\n",
      "Train Epoch: 0 [140800/162355 (87%)]\tLoss: 0.594108\n",
      "Train Epoch: 0 [147200/162355 (91%)]\tLoss: 0.434253\n",
      "Train Epoch: 0 [153600/162355 (95%)]\tLoss: 0.428331\n",
      "Train Epoch: 0 [160000/162355 (99%)]\tLoss: 0.570605\n",
      "Train Epoch: 1 [0/162355 (0%)]\tLoss: 0.470400\n",
      "Train Epoch: 1 [6400/162355 (4%)]\tLoss: 0.497132\n",
      "Train Epoch: 1 [12800/162355 (8%)]\tLoss: 0.385450\n",
      "Train Epoch: 1 [19200/162355 (12%)]\tLoss: 0.462021\n",
      "Train Epoch: 1 [25600/162355 (16%)]\tLoss: 0.342050\n",
      "Train Epoch: 1 [32000/162355 (20%)]\tLoss: 0.472480\n",
      "Train Epoch: 1 [38400/162355 (24%)]\tLoss: 0.409928\n",
      "Train Epoch: 1 [44800/162355 (28%)]\tLoss: 0.441303\n",
      "Train Epoch: 1 [51200/162355 (32%)]\tLoss: 0.417786\n",
      "Train Epoch: 1 [57600/162355 (35%)]\tLoss: 0.299045\n",
      "Train Epoch: 1 [64000/162355 (39%)]\tLoss: 0.355120\n",
      "Train Epoch: 1 [70400/162355 (43%)]\tLoss: 0.353250\n",
      "Train Epoch: 1 [76800/162355 (47%)]\tLoss: 0.599214\n",
      "Train Epoch: 1 [83200/162355 (51%)]\tLoss: 0.461468\n",
      "Train Epoch: 1 [89600/162355 (55%)]\tLoss: 0.380314\n",
      "Train Epoch: 1 [96000/162355 (59%)]\tLoss: 0.431632\n",
      "Train Epoch: 1 [102400/162355 (63%)]\tLoss: 0.382702\n",
      "Train Epoch: 1 [108800/162355 (67%)]\tLoss: 0.324498\n",
      "Train Epoch: 1 [115200/162355 (71%)]\tLoss: 0.463284\n",
      "Train Epoch: 1 [121600/162355 (75%)]\tLoss: 0.376703\n",
      "Train Epoch: 1 [128000/162355 (79%)]\tLoss: 0.404805\n",
      "Train Epoch: 1 [134400/162355 (83%)]\tLoss: 0.492812\n",
      "Train Epoch: 1 [140800/162355 (87%)]\tLoss: 0.465104\n",
      "Train Epoch: 1 [147200/162355 (91%)]\tLoss: 0.290262\n",
      "Train Epoch: 1 [153600/162355 (95%)]\tLoss: 0.499366\n",
      "Train Epoch: 1 [160000/162355 (99%)]\tLoss: 0.320246\n",
      "ROC AUC Score 8 hidden layers sigmoid: 0.6134508094275479\n"
     ]
    }
   ],
   "source": [
    "input_size=21\n",
    "output_size=2\n",
    "n_hidden=8 \n",
    "model_fnn=FC2Layer(input_size, n_hidden, output_size, .5, 'sigmoid')\n",
    "model_fnn=model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "for epoch in range(0, 2):\n",
    "    train(epoch, model_fnn, optimizer, verbose=True)\n",
    "auc_score = calculate_auc(model_fnn, test_loader)\n",
    "print(\"ROC AUC Score 8 hidden layers sigmoid:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b436dbb-d95d-4598-945d-6281e76cd9cf",
   "metadata": {},
   "source": [
    "3.I defined a CNN class with two fully connected layers and three hidden layers, conv1, relu, and maxpooling and trained the dataset for two epochs. There is also an embedding layer to handle categorical data separately from continuous data, where categorical data like MentalHealth which takes values from 0 to 30 and Smoker which takes 0 or 1 are assigned a lower dimension vector, in this case a scalar since most categorical variables are binary, to help reduce dimensionality. Then I put these through conv1 which learns spatial patterns from the categorical data, which is why CNN is helpful for our dataset because many of these features are related like smoking and general health, and CNN's ability to distinguish feature hierarchy means these features can contribute to learning about specific aspects of data at different scales in convoluted layers. I then apply Relu to reduce complexity of the high dimension data, since there are 16 out channels, one for each feature with categorical data, and max pool to perform feature selection and dimensionality reduction to prevent overfitting. Then I concatenate the resulting embedded output matrix with the continuous data to put through the fully connected layers then the Relu activation which sets negative values to 0 and introduces some nonlinearity and further prepares the data for predictions in the softmax layer, which assigns the inputs a probability that results in our label. This model has an accuracy of 86.58% on the testloader data set, most likley due to the hyperparameter of out channels, I chose one for each in the embedding layer so that each feature then contributes independently to the output and the model may be able to learn more about specific feature influences but also creates risk of overfitting so I use dimensionality reduction to mitigate the effects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "055ca067-9072-4a35-a4dc-a0f4ffdc590f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sw/z1pq8bn92ms8vtnzf3rkgfzh0000gn/T/ipykernel_85907/4164487157.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedded_outputs = [torch.tensor(embedded_output) for embedded_output in embedded_outputs]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/202944 (0%)]\tLoss: 0.597890\n",
      "Train Epoch: 0 [6400/202944 (3%)]\tLoss: 0.286653\n",
      "Train Epoch: 0 [12800/202944 (6%)]\tLoss: 0.260705\n",
      "Train Epoch: 0 [19200/202944 (9%)]\tLoss: 0.334589\n",
      "Train Epoch: 0 [25600/202944 (13%)]\tLoss: 0.359837\n",
      "Train Epoch: 0 [32000/202944 (16%)]\tLoss: 0.361104\n",
      "Train Epoch: 0 [38400/202944 (19%)]\tLoss: 0.283162\n",
      "Train Epoch: 0 [44800/202944 (22%)]\tLoss: 0.380380\n",
      "Train Epoch: 0 [51200/202944 (25%)]\tLoss: 0.353416\n",
      "Train Epoch: 0 [57600/202944 (28%)]\tLoss: 0.355080\n",
      "Train Epoch: 0 [64000/202944 (32%)]\tLoss: 0.343141\n",
      "Train Epoch: 0 [70400/202944 (35%)]\tLoss: 0.418286\n",
      "Train Epoch: 0 [76800/202944 (38%)]\tLoss: 0.265704\n",
      "Train Epoch: 0 [83200/202944 (41%)]\tLoss: 0.349559\n",
      "Train Epoch: 0 [89600/202944 (44%)]\tLoss: 0.401909\n",
      "Train Epoch: 0 [96000/202944 (47%)]\tLoss: 0.235442\n",
      "Train Epoch: 0 [102400/202944 (50%)]\tLoss: 0.262743\n",
      "Train Epoch: 0 [108800/202944 (54%)]\tLoss: 0.245238\n",
      "Train Epoch: 0 [115200/202944 (57%)]\tLoss: 0.184266\n",
      "Train Epoch: 0 [121600/202944 (60%)]\tLoss: 0.364223\n",
      "Train Epoch: 0 [128000/202944 (63%)]\tLoss: 0.311545\n",
      "Train Epoch: 0 [134400/202944 (66%)]\tLoss: 0.261124\n",
      "Train Epoch: 0 [140800/202944 (69%)]\tLoss: 0.405352\n",
      "Train Epoch: 0 [147200/202944 (73%)]\tLoss: 0.307541\n",
      "Train Epoch: 0 [153600/202944 (76%)]\tLoss: 0.495827\n",
      "Train Epoch: 0 [160000/202944 (79%)]\tLoss: 0.357198\n",
      "Train Epoch: 0 [166400/202944 (82%)]\tLoss: 0.347763\n",
      "Train Epoch: 0 [172800/202944 (85%)]\tLoss: 0.267664\n",
      "Train Epoch: 0 [179200/202944 (88%)]\tLoss: 0.367289\n",
      "Train Epoch: 0 [185600/202944 (91%)]\tLoss: 0.315885\n",
      "Train Epoch: 0 [192000/202944 (95%)]\tLoss: 0.575993\n",
      "Train Epoch: 0 [198400/202944 (98%)]\tLoss: 0.190268\n",
      "Epoch: 0 Average loss: 0.0052\n",
      "Train Epoch: 1 [0/202944 (0%)]\tLoss: 0.186222\n",
      "Train Epoch: 1 [6400/202944 (3%)]\tLoss: 0.394132\n",
      "Train Epoch: 1 [12800/202944 (6%)]\tLoss: 0.328815\n",
      "Train Epoch: 1 [19200/202944 (9%)]\tLoss: 0.286173\n",
      "Train Epoch: 1 [25600/202944 (13%)]\tLoss: 0.247136\n",
      "Train Epoch: 1 [32000/202944 (16%)]\tLoss: 0.355658\n",
      "Train Epoch: 1 [38400/202944 (19%)]\tLoss: 0.357861\n",
      "Train Epoch: 1 [44800/202944 (22%)]\tLoss: 0.376386\n",
      "Train Epoch: 1 [51200/202944 (25%)]\tLoss: 0.612362\n",
      "Train Epoch: 1 [57600/202944 (28%)]\tLoss: 0.229902\n",
      "Train Epoch: 1 [64000/202944 (32%)]\tLoss: 0.400071\n",
      "Train Epoch: 1 [70400/202944 (35%)]\tLoss: 0.361967\n",
      "Train Epoch: 1 [76800/202944 (38%)]\tLoss: 0.253976\n",
      "Train Epoch: 1 [83200/202944 (41%)]\tLoss: 0.327260\n",
      "Train Epoch: 1 [89600/202944 (44%)]\tLoss: 0.429544\n",
      "Train Epoch: 1 [96000/202944 (47%)]\tLoss: 0.311384\n",
      "Train Epoch: 1 [102400/202944 (50%)]\tLoss: 0.351329\n",
      "Train Epoch: 1 [108800/202944 (54%)]\tLoss: 0.455932\n",
      "Train Epoch: 1 [115200/202944 (57%)]\tLoss: 0.278058\n",
      "Train Epoch: 1 [121600/202944 (60%)]\tLoss: 0.310136\n",
      "Train Epoch: 1 [128000/202944 (63%)]\tLoss: 0.340865\n",
      "Train Epoch: 1 [134400/202944 (66%)]\tLoss: 0.254659\n",
      "Train Epoch: 1 [140800/202944 (69%)]\tLoss: 0.229001\n",
      "Train Epoch: 1 [147200/202944 (73%)]\tLoss: 0.258813\n",
      "Train Epoch: 1 [153600/202944 (76%)]\tLoss: 0.346526\n",
      "Train Epoch: 1 [160000/202944 (79%)]\tLoss: 0.290041\n",
      "Train Epoch: 1 [166400/202944 (82%)]\tLoss: 0.312376\n",
      "Train Epoch: 1 [172800/202944 (85%)]\tLoss: 0.295578\n",
      "Train Epoch: 1 [179200/202944 (88%)]\tLoss: 0.421595\n",
      "Train Epoch: 1 [185600/202944 (91%)]\tLoss: 0.312911\n",
      "Train Epoch: 1 [192000/202944 (95%)]\tLoss: 0.318606\n",
      "Train Epoch: 1 [198400/202944 (98%)]\tLoss: 0.302994\n",
      "Epoch: 1 Average loss: 0.0051\n",
      "Accuracy on test set: 86.58%\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_sizes, n_continuous, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embedding_layers = nn.ModuleList([nn.Embedding(num_embeddings=16, embedding_dim=embedding_dim) for size, embedding_dim in embedding_sizes])\n",
    "        self.conv1 = nn.Conv1d(in_channels=len(embedding_sizes), out_channels=16, kernel_size=1)\n",
    "        self.fc1 = nn.Linear(16+n_continuous,64)\n",
    "        self.fc2 = nn.Linear(64,output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        embedded_outputs=[]\n",
    "        for i in range(16):\n",
    "            embedding_layer = self.embedding_layers[i]\n",
    "            embedded_outputs.append(embedding_layer(x_cat[:, i].long()))\n",
    "        embedded_outputs = [torch.tensor(embedded_output) for embedded_output in embedded_outputs]\n",
    "        embedded_outputs = torch.stack(embedded_outputs, dim=1)\n",
    "        embedded_outputs = embedded_outputs.unsqueeze(1)\n",
    "        embedded_outputs = embedded_outputs.view(embedded_outputs.size(0), -1)\n",
    "        embedded_outputs = embedded_outputs.transpose(0, 1)\n",
    "        conv_out = self.conv1(embedded_outputs)\n",
    "        conv_out = self.relu(conv_out)\n",
    "        conv_out = torch.max_pool1d(conv_out, kernel_size=1)\n",
    "        conv_out = conv_out.view(conv_out.size(0), -1) \n",
    "        conv_out=torch.transpose(conv_out, 0, 1)\n",
    "        combined = torch.cat([conv_out, x_cont], dim=1)\n",
    "        fc1_out = self.fc1(combined)\n",
    "        fc1_out = self.relu(fc1_out)\n",
    "        output = self.fc2(fc1_out)\n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "data=pd.read_csv('/Users/marieqi/Downloads/diabetes.csv')\n",
    "df=data.dropna()\n",
    "scaler=StandardScaler()\n",
    "cols=['BMI', 'MentalHealth','PhysicalHealth', 'AgeBracket', 'EducationBracket', 'IncomeBracket', 'Zodiac']\n",
    "df[cols]=scaler.fit_transform(df[cols])\n",
    "scaler= StandardScaler()\n",
    "df_continuous=df[['BMI', 'AgeBracket', 'EducationBracket', 'IncomeBracket', 'Zodiac']].values\n",
    "df_categorical= df[['HighBP','HighChol','Smoker','Stroke','Myocardial','PhysActivity','Fruit','Vegetables','HeavyDrinker','HasHealthcare','NotAbleToAffordDoctor','GeneralHealth','MentalHealth','PhysicalHealth','HardToClimbStairs','BiologicalSex']].values\n",
    "df_categorical= torch.tensor(df_categorical, dtype=torch.long)\n",
    "df_continuous= torch.tensor(scaler.fit_transform(df_continuous), dtype=torch.float32)\n",
    "embedding_sizes=[(2, 1), (2, 1), (2, 1), (2, 1),(2, 1), (2, 1),(2, 1), (2, 1),(2, 1), (2, 1),(2, 1), (5, 1),(30,1),(30,1),(2, 1),(2, 1)]\n",
    "input_size_continuous = df_continuous.shape[1]\n",
    "output_size= 2 \n",
    "model=CNN(input_size=len(embedding_sizes), embedding_sizes=embedding_sizes, n_continuous=input_size_continuous, output_size=output_size)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(model.parameters(), lr=0.001)\n",
    "X_train_cat, X_test_cat, X_train_cont, X_test_cont, y_train, y_test = train_test_split(df_categorical, df_continuous, df['Diabetes'], test_size=0.2, random_state=42)\n",
    "train_dataset=TensorDataset(X_train_cat, X_train_cont, torch.tensor(y_train.values))\n",
    "test_dataset=TensorDataset(X_test_cat, X_test_cont, torch.tensor(y_test.values))\n",
    "train_loader=DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader=DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "num_epochs=2\n",
    "def train1(epoch, model, optimizer, train_loader, perm=torch.arange(0, 784).long(), verbose=False):\n",
    "    model.train()\n",
    "    epoch_loss=0\n",
    "    losses=[]\n",
    "    for batch_idx, (cat_data, cont_data, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs=model(cat_data,cont_data)\n",
    "        loss=criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss+=loss.item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            losses.append(loss.detach())\n",
    "            if verbose:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch,batch_idx*len(cat_data),len(train_loader.dataset),\n",
    "                    100.*batch_idx/len(train_loader), loss.item()))\n",
    "    epoch_loss /= len(train_loader.dataset)\n",
    "    print('Epoch: {} Average loss: {:.4f}'.format(epoch, epoch_loss))\n",
    "    return losses\n",
    "for epoch in range(num_epochs):\n",
    "    train_losses = train1(epoch, model, optimizer, train_loader, verbose=True)\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for cat_data, cont_data, labels in test_loader:\n",
    "            outputs=model(cat_data, cont_data)\n",
    "            _, predicted=torch.max(outputs.data, 1)\n",
    "            total+=labels.size(0)\n",
    "            correct+=(predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy=correct/total\n",
    "    print('Accuracy on test set: {:.2f}%'.format(100*accuracy))\n",
    "    return accuracy\n",
    "accuracy=evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e900bd12-5777-4d2b-9097-8dad0d85fe55",
   "metadata": {},
   "source": [
    "4.The feedforward neural network with one hidden layer uses dropout to handle class imbalance and relu activation as for one hidden layer, this reduces dimensionality and creates network sparsity which helps aid convergence. First a fully connected layer uses linear transformation with weights and biases, adjusted with respect to a learning rate to prepare the data for prediction, then the activation function introduces non linearity and random dropout happens afterwards to help mitigate the effects of class imbalance. This repeats three times before the output is fed to the softmax function for prediction. The RMSE of the model is 34.4805 which means predictions were off by a very wide margin, however the range of the data is large, with the maximum being 97, and RMSE is sensitive to outliers so it may not be the most robust metric to evaluate the model. The RMSE of the model with sigmoid instead of RELU is 34.5014, which is almost exactly the same so RMSE does not depend on activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5ab420e9-0667-4477-830b-8187ecb11a4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/162355 (0%)]\tLoss: 5.031344\n",
      "Train Epoch: 0 [6400/162355 (4%)]\tLoss: 4.878225\n",
      "Train Epoch: 0 [12800/162355 (8%)]\tLoss: 4.579885\n",
      "Train Epoch: 0 [19200/162355 (12%)]\tLoss: 4.563241\n",
      "Train Epoch: 0 [25600/162355 (16%)]\tLoss: 4.584604\n",
      "Train Epoch: 0 [32000/162355 (20%)]\tLoss: 4.380042\n",
      "Train Epoch: 0 [38400/162355 (24%)]\tLoss: 4.234180\n",
      "Train Epoch: 0 [44800/162355 (28%)]\tLoss: 4.202841\n",
      "Train Epoch: 0 [51200/162355 (32%)]\tLoss: 4.149135\n",
      "Train Epoch: 0 [57600/162355 (35%)]\tLoss: 4.102171\n",
      "Train Epoch: 0 [64000/162355 (39%)]\tLoss: 4.109626\n",
      "Train Epoch: 0 [70400/162355 (43%)]\tLoss: 4.018848\n",
      "Train Epoch: 0 [76800/162355 (47%)]\tLoss: 3.927942\n",
      "Train Epoch: 0 [83200/162355 (51%)]\tLoss: 3.902678\n",
      "Train Epoch: 0 [89600/162355 (55%)]\tLoss: 3.791308\n",
      "Train Epoch: 0 [96000/162355 (59%)]\tLoss: 3.745782\n",
      "Train Epoch: 0 [102400/162355 (63%)]\tLoss: 3.662572\n",
      "Train Epoch: 0 [108800/162355 (67%)]\tLoss: 3.818199\n",
      "Train Epoch: 0 [115200/162355 (71%)]\tLoss: 3.627409\n",
      "Train Epoch: 0 [121600/162355 (75%)]\tLoss: 3.476642\n",
      "Train Epoch: 0 [128000/162355 (79%)]\tLoss: 3.551855\n",
      "Train Epoch: 0 [134400/162355 (83%)]\tLoss: 3.472768\n",
      "Train Epoch: 0 [140800/162355 (87%)]\tLoss: 3.546116\n",
      "Train Epoch: 0 [147200/162355 (91%)]\tLoss: 3.465754\n",
      "Train Epoch: 0 [153600/162355 (95%)]\tLoss: 3.564938\n",
      "Train Epoch: 0 [160000/162355 (99%)]\tLoss: 3.591798\n",
      "Train Epoch: 1 [0/162355 (0%)]\tLoss: 3.721836\n",
      "Train Epoch: 1 [6400/162355 (4%)]\tLoss: 3.500055\n",
      "Train Epoch: 1 [12800/162355 (8%)]\tLoss: 3.441775\n",
      "Train Epoch: 1 [19200/162355 (12%)]\tLoss: 3.513893\n",
      "Train Epoch: 1 [25600/162355 (16%)]\tLoss: 3.484669\n",
      "Train Epoch: 1 [32000/162355 (20%)]\tLoss: 3.523400\n",
      "Train Epoch: 1 [38400/162355 (24%)]\tLoss: 3.397312\n",
      "Train Epoch: 1 [44800/162355 (28%)]\tLoss: 3.555901\n",
      "Train Epoch: 1 [51200/162355 (32%)]\tLoss: 3.450002\n",
      "Train Epoch: 1 [57600/162355 (35%)]\tLoss: 3.309519\n",
      "Train Epoch: 1 [64000/162355 (39%)]\tLoss: 3.381724\n",
      "Train Epoch: 1 [70400/162355 (43%)]\tLoss: 3.394521\n",
      "Train Epoch: 1 [76800/162355 (47%)]\tLoss: 3.381579\n",
      "Train Epoch: 1 [83200/162355 (51%)]\tLoss: 3.340293\n",
      "Train Epoch: 1 [89600/162355 (55%)]\tLoss: 3.391135\n",
      "Train Epoch: 1 [96000/162355 (59%)]\tLoss: 3.273620\n",
      "Train Epoch: 1 [102400/162355 (63%)]\tLoss: 3.449146\n",
      "Train Epoch: 1 [108800/162355 (67%)]\tLoss: 3.522780\n",
      "Train Epoch: 1 [115200/162355 (71%)]\tLoss: 3.342592\n",
      "Train Epoch: 1 [121600/162355 (75%)]\tLoss: 3.296786\n",
      "Train Epoch: 1 [128000/162355 (79%)]\tLoss: 3.282637\n",
      "Train Epoch: 1 [134400/162355 (83%)]\tLoss: 3.365895\n",
      "Train Epoch: 1 [140800/162355 (87%)]\tLoss: 3.488444\n",
      "Train Epoch: 1 [147200/162355 (91%)]\tLoss: 3.239436\n",
      "Train Epoch: 1 [153600/162355 (95%)]\tLoss: 3.119637\n",
      "Train Epoch: 1 [160000/162355 (99%)]\tLoss: 3.225578\n",
      "RMSE tensor(34.4805)\n",
      "Train Epoch: 0 [0/162355 (0%)]\tLoss: 4.856491\n",
      "Train Epoch: 0 [6400/162355 (4%)]\tLoss: 4.927934\n",
      "Train Epoch: 0 [12800/162355 (8%)]\tLoss: 4.721104\n",
      "Train Epoch: 0 [19200/162355 (12%)]\tLoss: 4.551903\n",
      "Train Epoch: 0 [25600/162355 (16%)]\tLoss: 4.578555\n",
      "Train Epoch: 0 [32000/162355 (20%)]\tLoss: 4.382790\n",
      "Train Epoch: 0 [38400/162355 (24%)]\tLoss: 4.286834\n",
      "Train Epoch: 0 [44800/162355 (28%)]\tLoss: 4.154713\n",
      "Train Epoch: 0 [51200/162355 (32%)]\tLoss: 3.990938\n",
      "Train Epoch: 0 [57600/162355 (35%)]\tLoss: 4.098946\n",
      "Train Epoch: 0 [64000/162355 (39%)]\tLoss: 4.102112\n",
      "Train Epoch: 0 [70400/162355 (43%)]\tLoss: 3.880947\n",
      "Train Epoch: 0 [76800/162355 (47%)]\tLoss: 3.861784\n",
      "Train Epoch: 0 [83200/162355 (51%)]\tLoss: 3.715747\n",
      "Train Epoch: 0 [89600/162355 (55%)]\tLoss: 3.854753\n",
      "Train Epoch: 0 [96000/162355 (59%)]\tLoss: 3.936010\n",
      "Train Epoch: 0 [102400/162355 (63%)]\tLoss: 3.832751\n",
      "Train Epoch: 0 [108800/162355 (67%)]\tLoss: 3.980896\n",
      "Train Epoch: 0 [115200/162355 (71%)]\tLoss: 3.722152\n",
      "Train Epoch: 0 [121600/162355 (75%)]\tLoss: 3.599246\n",
      "Train Epoch: 0 [128000/162355 (79%)]\tLoss: 3.558171\n",
      "Train Epoch: 0 [134400/162355 (83%)]\tLoss: 3.573566\n",
      "Train Epoch: 0 [140800/162355 (87%)]\tLoss: 3.637195\n",
      "Train Epoch: 0 [147200/162355 (91%)]\tLoss: 3.547489\n",
      "Train Epoch: 0 [153600/162355 (95%)]\tLoss: 3.641637\n",
      "Train Epoch: 0 [160000/162355 (99%)]\tLoss: 3.791810\n",
      "Train Epoch: 1 [0/162355 (0%)]\tLoss: 3.486977\n",
      "Train Epoch: 1 [6400/162355 (4%)]\tLoss: 3.684731\n",
      "Train Epoch: 1 [12800/162355 (8%)]\tLoss: 3.518810\n",
      "Train Epoch: 1 [19200/162355 (12%)]\tLoss: 3.499403\n",
      "Train Epoch: 1 [25600/162355 (16%)]\tLoss: 3.673573\n",
      "Train Epoch: 1 [32000/162355 (20%)]\tLoss: 3.271528\n",
      "Train Epoch: 1 [38400/162355 (24%)]\tLoss: 3.516016\n",
      "Train Epoch: 1 [44800/162355 (28%)]\tLoss: 3.502844\n",
      "Train Epoch: 1 [51200/162355 (32%)]\tLoss: 3.577075\n",
      "Train Epoch: 1 [57600/162355 (35%)]\tLoss: 3.458011\n",
      "Train Epoch: 1 [64000/162355 (39%)]\tLoss: 3.447301\n",
      "Train Epoch: 1 [70400/162355 (43%)]\tLoss: 3.371492\n",
      "Train Epoch: 1 [76800/162355 (47%)]\tLoss: 3.303989\n",
      "Train Epoch: 1 [83200/162355 (51%)]\tLoss: 3.464019\n",
      "Train Epoch: 1 [89600/162355 (55%)]\tLoss: 3.380112\n",
      "Train Epoch: 1 [96000/162355 (59%)]\tLoss: 3.403661\n",
      "Train Epoch: 1 [102400/162355 (63%)]\tLoss: 3.408834\n",
      "Train Epoch: 1 [108800/162355 (67%)]\tLoss: 3.555313\n",
      "Train Epoch: 1 [115200/162355 (71%)]\tLoss: 3.498668\n",
      "Train Epoch: 1 [121600/162355 (75%)]\tLoss: 3.154466\n",
      "Train Epoch: 1 [128000/162355 (79%)]\tLoss: 3.324763\n",
      "Train Epoch: 1 [134400/162355 (83%)]\tLoss: 3.299797\n",
      "Train Epoch: 1 [140800/162355 (87%)]\tLoss: 3.257428\n",
      "Train Epoch: 1 [147200/162355 (91%)]\tLoss: 3.435116\n",
      "Train Epoch: 1 [153600/162355 (95%)]\tLoss: 3.305387\n",
      "Train Epoch: 1 [160000/162355 (99%)]\tLoss: 3.307893\n",
      "RMSE tensor(34.5014)\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('/Users/marieqi/Downloads/diabetes.csv')\n",
    "df=data.dropna()\n",
    "scaler=StandardScaler()\n",
    "Y=df['BMI']\n",
    "X=df.drop(columns=['BMI'])\n",
    "X_train, X_test, Y_train, Y_test=train_test_split(X,Y,test_size=.2, random_state=42)\n",
    "X_train, X_val, Y_train, y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n",
    "train_dataset=CustomDataset(X_train,Y_train)\n",
    "test_dataset=CustomDataset(X_test,Y_test)\n",
    "train_loader=DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader=DataLoader(test_dataset, batch_size=100, shuffle=True)\n",
    "def train(epoch, model, optimizer, perm=torch.arange(0, 21).long(), verbose=False):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        losses = []\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(-1, 21)\n",
    "            data = data[:, perm]\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 100 == 0:\n",
    "                losses.append(loss.detach())\n",
    "                if verbose :\n",
    "                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                        100. * batch_idx / len(train_loader), loss.item()))\n",
    "                    \n",
    "        return losses\n",
    "def calculate_rmse(model, test_loader):\n",
    "    model.eval()  \n",
    "    rmse=0.0\n",
    "    total_samples=0\n",
    "    with torch.no_grad():\n",
    "        for inputs,targets in test_loader:\n",
    "            outputs=model(inputs) \n",
    "            squared_diff=torch.pow(outputs-targets.unsqueeze(1), 2)\n",
    "            mse=torch.mean(squared_diff)\n",
    "            rmse+=torch.sqrt(mse)\n",
    "            total_samples+=len(targets)\n",
    "    rmse/=len(test_loader)\n",
    "    return rmse\n",
    "input_size=21\n",
    "output_size=100\n",
    "n_hidden=1 \n",
    "model_fnn=FC2Layer(input_size, n_hidden, output_size, .5, 'relu')\n",
    "model_fnn=model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "for epoch in range(0, 2):\n",
    "    train(epoch, model_fnn, optimizer, verbose=True)\n",
    "rmse=calculate_rmse(model_fnn,test_loader)\n",
    "print('RMSE',rmse)\n",
    "input_size=21\n",
    "output_size=100\n",
    "n_hidden=1 \n",
    "model_fnn=FC2Layer(input_size, n_hidden, output_size, .5, 'sigmoid')\n",
    "model_fnn=model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "for epoch in range(0, 2):\n",
    "    train(epoch, model_fnn, optimizer, verbose=True)\n",
    "rmse=calculate_rmse(model_fnn,test_loader)\n",
    "print('RMSE',rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5c67ae-948b-4c49-ab06-cbc3e42ef1bb",
   "metadata": {},
   "source": [
    "5. I used the same convolutional deep neural network as in question 3 to to predict BMI and I used iterative methods to tune the learning rate hyperparameter of the ADAM optimizer in lieu of grid search. I initialized an array of hyperparameters and trained the model on all of them, keeping all other parameters constant, to see the effect of learning rate on RMSE, through this I find that the best RMSE is 37.5484 with a learning rate parameter of .01, whereas .1 results in a slightly higher RMSE of 37.8679, and .5 results in a rmse of about 39. Learning rate of the ADAM optimizer I used is updated iteratively from a starting point. The algorithm uses gradient descent and adjusts the learning rate based on the mean and variance of the gradients therefore it is expected to converge to the optimal assuming it is not set too high that it overshoots or too low that convergence does not happen within the alotted iterations. Therfore ensuring parameters are updated with ideal step size minimizes RMSE by minimizing distance from predicted to actual points.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "68e06a7c-621f-493f-a032-e1a1c379d6f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sw/z1pq8bn92ms8vtnzf3rkgfzh0000gn/T/ipykernel_85907/873824480.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedded_outputs = [torch.tensor(embedded_output) for embedded_output in embedded_outputs]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/202944 (0%)]\tLoss: 4.677636\n",
      "Train Epoch: 0 [10000/202944 (5%)]\tLoss: 3.158914\n",
      "Train Epoch: 0 [20000/202944 (10%)]\tLoss: 3.071005\n",
      "Train Epoch: 0 [30000/202944 (15%)]\tLoss: 3.044076\n",
      "Train Epoch: 0 [40000/202944 (20%)]\tLoss: 3.120671\n",
      "Train Epoch: 0 [50000/202944 (25%)]\tLoss: 3.089694\n",
      "Train Epoch: 0 [60000/202944 (30%)]\tLoss: 3.068555\n",
      "Train Epoch: 0 [70000/202944 (34%)]\tLoss: 3.183105\n",
      "Train Epoch: 0 [80000/202944 (39%)]\tLoss: 3.025813\n",
      "Train Epoch: 0 [90000/202944 (44%)]\tLoss: 3.203900\n",
      "Train Epoch: 0 [100000/202944 (49%)]\tLoss: 3.072562\n",
      "Train Epoch: 0 [110000/202944 (54%)]\tLoss: 3.044748\n",
      "Train Epoch: 0 [120000/202944 (59%)]\tLoss: 3.198979\n",
      "Train Epoch: 0 [130000/202944 (64%)]\tLoss: 3.078929\n",
      "Train Epoch: 0 [140000/202944 (69%)]\tLoss: 3.281534\n",
      "Train Epoch: 0 [150000/202944 (74%)]\tLoss: 3.067777\n",
      "Train Epoch: 0 [160000/202944 (79%)]\tLoss: 2.976465\n",
      "Train Epoch: 0 [170000/202944 (84%)]\tLoss: 3.336936\n",
      "Train Epoch: 0 [180000/202944 (89%)]\tLoss: 3.124246\n",
      "Train Epoch: 0 [190000/202944 (94%)]\tLoss: 3.148195\n",
      "Train Epoch: 0 [200000/202944 (99%)]\tLoss: 3.059289\n",
      "Epoch: 0 Average loss: 0.0316\n",
      "Train Epoch: 1 [0/202944 (0%)]\tLoss: 3.214802\n",
      "Train Epoch: 1 [10000/202944 (5%)]\tLoss: 3.227239\n",
      "Train Epoch: 1 [20000/202944 (10%)]\tLoss: 3.191106\n",
      "Train Epoch: 1 [30000/202944 (15%)]\tLoss: 3.062903\n",
      "Train Epoch: 1 [40000/202944 (20%)]\tLoss: 3.058193\n",
      "Train Epoch: 1 [50000/202944 (25%)]\tLoss: 3.187351\n",
      "Train Epoch: 1 [60000/202944 (30%)]\tLoss: 3.029397\n",
      "Train Epoch: 1 [70000/202944 (34%)]\tLoss: 3.110393\n",
      "Train Epoch: 1 [80000/202944 (39%)]\tLoss: 3.520000\n",
      "Train Epoch: 1 [90000/202944 (44%)]\tLoss: 3.174416\n",
      "Train Epoch: 1 [100000/202944 (49%)]\tLoss: 3.001042\n",
      "Train Epoch: 1 [110000/202944 (54%)]\tLoss: 3.180983\n",
      "Train Epoch: 1 [120000/202944 (59%)]\tLoss: 3.291021\n",
      "Train Epoch: 1 [130000/202944 (64%)]\tLoss: 3.069832\n",
      "Train Epoch: 1 [140000/202944 (69%)]\tLoss: 3.156155\n",
      "Train Epoch: 1 [150000/202944 (74%)]\tLoss: 3.014655\n",
      "Train Epoch: 1 [160000/202944 (79%)]\tLoss: 3.037289\n",
      "Train Epoch: 1 [170000/202944 (84%)]\tLoss: 3.320134\n",
      "Train Epoch: 1 [180000/202944 (89%)]\tLoss: 3.055502\n",
      "Train Epoch: 1 [190000/202944 (94%)]\tLoss: 3.192311\n",
      "Train Epoch: 1 [200000/202944 (99%)]\tLoss: 3.112099\n",
      "Epoch: 1 Average loss: 0.0315\n",
      "RMSE tensor(37.8679)\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_sizes, n_continuous, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embedding_layers = nn.ModuleList([nn.Embedding(num_embeddings=17, embedding_dim=embedding_dim) for size, embedding_dim in embedding_sizes])\n",
    "        self.conv1 = nn.Conv1d(in_channels=11, out_channels=11, kernel_size=1)\n",
    "        self.fc1 = nn.Linear(11+n_continuous,64)\n",
    "        self.fc2 = nn.Linear(64,output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        embedded_outputs=[]\n",
    "        for i in range(11):\n",
    "            embedding_layer = self.embedding_layers[i]\n",
    "            embedded_outputs.append(embedding_layer(x_cat[:, i].long()))\n",
    "        embedded_outputs = [torch.tensor(embedded_output) for embedded_output in embedded_outputs]\n",
    "        embedded_outputs = torch.stack(embedded_outputs, dim=1)\n",
    "        embedded_outputs = embedded_outputs.unsqueeze(1)\n",
    "        embedded_outputs = embedded_outputs.view(embedded_outputs.size(0), -1)\n",
    "        embedded_outputs = embedded_outputs.transpose(0, 1)\n",
    "        conv_out = self.conv1(embedded_outputs)\n",
    "        conv_out = self.relu(conv_out)\n",
    "        conv_out = torch.max_pool1d(conv_out, kernel_size=1)\n",
    "        conv_out = conv_out.view(conv_out.size(0), -1) \n",
    "        conv_out=torch.transpose(conv_out, 0, 1)\n",
    "        combined = torch.cat([conv_out, x_cont], dim=1)\n",
    "        fc1_out = self.fc1(combined)\n",
    "        fc1_out = self.relu(fc1_out)\n",
    "        output = self.fc2(fc1_out)\n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "data=pd.read_csv('/Users/marieqi/Downloads/diabetes.csv')\n",
    "df=data.dropna()\n",
    "scaler= StandardScaler()\n",
    "df_continuous=df[['AgeBracket', 'EducationBracket', 'IncomeBracket', 'Zodiac']].values\n",
    "df_categorical= df[['Diabetes','HighBP','HighChol','Smoker','Stroke','Myocardial','PhysActivity','Fruit','Vegetables','HeavyDrinker','HasHealthcare','NotAbleToAffordDoctor','GeneralHealth','MentalHealth','PhysicalHealth','HardToClimbStairs','BiologicalSex']].values\n",
    "df_categorical= torch.tensor(df_categorical, dtype=torch.long)\n",
    "df_continuous= torch.tensor(scaler.fit_transform(df_continuous), dtype=torch.float32)\n",
    "embedding_sizes=[(2, 1),(2, 1), (2, 1), (2, 1), (2, 1),(2, 1), (2, 1),(2, 1), (2, 1),(2, 1), (2, 1),(2, 1), (5, 1),(30,1),(30,1),(2, 1),(2, 1)]\n",
    "input_size_continuous = df_continuous.shape[1]\n",
    "output_size=100\n",
    "model=CNN(input_size=len(embedding_sizes), embedding_sizes=embedding_sizes, n_continuous=input_size_continuous, output_size=output_size)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(model.parameters(), lr=0.1)\n",
    "X_train_cat, X_test_cat, X_train_cont, X_test_cont, y_train, y_test = train_test_split(df_categorical, df_continuous, df['BMI'], test_size=0.2, random_state=42)\n",
    "train_dataset=TensorDataset(X_train_cat, X_train_cont, torch.tensor(y_train.values))\n",
    "test_dataset=TensorDataset(X_test_cat, X_test_cont, torch.tensor(y_test.values))\n",
    "train_loader=DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "test_loader=DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
    "num_epochs=2\n",
    "def train1(epoch, model, optimizer, train_loader, perm=torch.arange(0, 784).long(), verbose=False):\n",
    "    model.train()\n",
    "    epoch_loss=0\n",
    "    losses=[]\n",
    "    for batch_idx, (cat_data, cont_data, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs=model(cat_data,cont_data)\n",
    "        labels=labels.long()\n",
    "        loss=criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss+=loss.item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            losses.append(loss.detach())\n",
    "            if verbose:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch,batch_idx*len(cat_data),len(train_loader.dataset),\n",
    "                    100.*batch_idx/len(train_loader), loss.item()))\n",
    "    epoch_loss /= len(train_loader.dataset)\n",
    "    print('Epoch: {} Average loss: {:.4f}'.format(epoch, epoch_loss))\n",
    "    return losses\n",
    "for epoch in range(num_epochs):\n",
    "    train_losses = train1(epoch, model, optimizer, train_loader, verbose=True)\n",
    "for cat_batch, cont_batch,labels_batch in test_loader:\n",
    "    combined_batch = torch.cat((cat_batch, cont_batch), dim=1)\n",
    "def calculate_rmse(model, test_loader):\n",
    "    model.eval()  \n",
    "    rmse=0.0\n",
    "    total_samples=0\n",
    "    with torch.no_grad():\n",
    "        for cat_batch, cont_batch, labels_batch in test_loader:\n",
    "            combined_batch = torch.cat((cat_batch, cont_batch), dim=1)\n",
    "            outputs = model(cat_batch,cont_batch)\n",
    "            squared_diff = torch.pow(outputs - labels_batch.unsqueeze(1), 2)\n",
    "            mse = torch.mean(squared_diff)\n",
    "            rmse += torch.sqrt(mse)\n",
    "            total_samples += len(labels_batch)\n",
    "    rmse /= len(test_loader)\n",
    "    return rmse\n",
    "rmse=calculate_rmse(model,test_loader)\n",
    "print('RMSE',rmse) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5aef4580-6906-4d3c-9c97-4929657a3315",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sw/z1pq8bn92ms8vtnzf3rkgfzh0000gn/T/ipykernel_85907/873824480.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedded_outputs = [torch.tensor(embedded_output) for embedded_output in embedded_outputs]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/202944 (0%)]\tLoss: 4.637815\n",
      "Train Epoch: 0 [10000/202944 (5%)]\tLoss: 3.065700\n",
      "Train Epoch: 0 [20000/202944 (10%)]\tLoss: 3.093155\n",
      "Train Epoch: 0 [30000/202944 (15%)]\tLoss: 3.010359\n",
      "Train Epoch: 0 [40000/202944 (20%)]\tLoss: 3.181977\n",
      "Train Epoch: 0 [50000/202944 (25%)]\tLoss: 3.034553\n",
      "Train Epoch: 0 [60000/202944 (30%)]\tLoss: 2.972929\n",
      "Train Epoch: 0 [70000/202944 (34%)]\tLoss: 3.069749\n",
      "Train Epoch: 0 [80000/202944 (39%)]\tLoss: 3.004210\n",
      "Train Epoch: 0 [90000/202944 (44%)]\tLoss: 3.112868\n",
      "Train Epoch: 0 [100000/202944 (49%)]\tLoss: 2.932541\n",
      "Train Epoch: 0 [110000/202944 (54%)]\tLoss: 3.179209\n",
      "Train Epoch: 0 [120000/202944 (59%)]\tLoss: 3.032987\n",
      "Train Epoch: 0 [130000/202944 (64%)]\tLoss: 3.014752\n",
      "Train Epoch: 0 [140000/202944 (69%)]\tLoss: 2.952281\n",
      "Train Epoch: 0 [150000/202944 (74%)]\tLoss: 3.094878\n",
      "Train Epoch: 0 [160000/202944 (79%)]\tLoss: 2.975713\n",
      "Train Epoch: 0 [170000/202944 (84%)]\tLoss: 3.264859\n",
      "Train Epoch: 0 [180000/202944 (89%)]\tLoss: 3.191909\n",
      "Train Epoch: 0 [190000/202944 (94%)]\tLoss: 3.057717\n",
      "Train Epoch: 0 [200000/202944 (99%)]\tLoss: 3.041134\n",
      "Epoch: 0 Average loss: 0.0308\n",
      "Train Epoch: 1 [0/202944 (0%)]\tLoss: 4.609190\n",
      "Train Epoch: 1 [10000/202944 (5%)]\tLoss: 3.177555\n",
      "Train Epoch: 1 [20000/202944 (10%)]\tLoss: 3.062963\n",
      "Train Epoch: 1 [30000/202944 (15%)]\tLoss: 3.080010\n",
      "Train Epoch: 1 [40000/202944 (20%)]\tLoss: 3.157140\n",
      "Train Epoch: 1 [50000/202944 (25%)]\tLoss: 3.103351\n",
      "Train Epoch: 1 [60000/202944 (30%)]\tLoss: 3.183144\n",
      "Train Epoch: 1 [70000/202944 (34%)]\tLoss: 3.115757\n",
      "Train Epoch: 1 [80000/202944 (39%)]\tLoss: 3.175096\n",
      "Train Epoch: 1 [90000/202944 (44%)]\tLoss: 2.970688\n",
      "Train Epoch: 1 [100000/202944 (49%)]\tLoss: 3.179747\n",
      "Train Epoch: 1 [110000/202944 (54%)]\tLoss: 3.214844\n",
      "Train Epoch: 1 [120000/202944 (59%)]\tLoss: 3.070676\n",
      "Train Epoch: 1 [130000/202944 (64%)]\tLoss: 3.231246\n",
      "Train Epoch: 1 [140000/202944 (69%)]\tLoss: 3.135827\n",
      "Train Epoch: 1 [150000/202944 (74%)]\tLoss: 3.108461\n",
      "Train Epoch: 1 [160000/202944 (79%)]\tLoss: 3.017082\n",
      "Train Epoch: 1 [170000/202944 (84%)]\tLoss: 3.068845\n",
      "Train Epoch: 1 [180000/202944 (89%)]\tLoss: 2.975638\n",
      "Train Epoch: 1 [190000/202944 (94%)]\tLoss: 3.093776\n",
      "Train Epoch: 1 [200000/202944 (99%)]\tLoss: 3.194203\n",
      "Epoch: 1 Average loss: 0.0308\n",
      "Train Epoch: 0 [0/202944 (0%)]\tLoss: 4.684625\n",
      "Train Epoch: 0 [10000/202944 (5%)]\tLoss: 3.188672\n",
      "Train Epoch: 0 [20000/202944 (10%)]\tLoss: 3.078487\n",
      "Train Epoch: 0 [30000/202944 (15%)]\tLoss: 3.143960\n",
      "Train Epoch: 0 [40000/202944 (20%)]\tLoss: 3.207368\n",
      "Train Epoch: 0 [50000/202944 (25%)]\tLoss: 2.983137\n",
      "Train Epoch: 0 [60000/202944 (30%)]\tLoss: 3.122024\n",
      "Train Epoch: 0 [70000/202944 (34%)]\tLoss: 2.977207\n",
      "Train Epoch: 0 [80000/202944 (39%)]\tLoss: 3.086879\n",
      "Train Epoch: 0 [90000/202944 (44%)]\tLoss: 2.929010\n",
      "Train Epoch: 0 [100000/202944 (49%)]\tLoss: 2.959054\n",
      "Train Epoch: 0 [110000/202944 (54%)]\tLoss: 3.099179\n",
      "Train Epoch: 0 [120000/202944 (59%)]\tLoss: 3.020561\n",
      "Train Epoch: 0 [130000/202944 (64%)]\tLoss: 3.159421\n",
      "Train Epoch: 0 [140000/202944 (69%)]\tLoss: 3.083315\n",
      "Train Epoch: 0 [150000/202944 (74%)]\tLoss: 3.036421\n",
      "Train Epoch: 0 [160000/202944 (79%)]\tLoss: 3.039176\n",
      "Train Epoch: 0 [170000/202944 (84%)]\tLoss: 3.251782\n",
      "Train Epoch: 0 [180000/202944 (89%)]\tLoss: 3.137691\n",
      "Train Epoch: 0 [190000/202944 (94%)]\tLoss: 3.130784\n",
      "Train Epoch: 0 [200000/202944 (99%)]\tLoss: 3.168121\n",
      "Epoch: 0 Average loss: 0.0310\n",
      "Train Epoch: 1 [0/202944 (0%)]\tLoss: 4.594328\n",
      "Train Epoch: 1 [10000/202944 (5%)]\tLoss: 3.092493\n",
      "Train Epoch: 1 [20000/202944 (10%)]\tLoss: 3.052832\n",
      "Train Epoch: 1 [30000/202944 (15%)]\tLoss: 2.891244\n",
      "Train Epoch: 1 [40000/202944 (20%)]\tLoss: 3.153454\n",
      "Train Epoch: 1 [50000/202944 (25%)]\tLoss: 3.156923\n",
      "Train Epoch: 1 [60000/202944 (30%)]\tLoss: 3.111242\n",
      "Train Epoch: 1 [70000/202944 (34%)]\tLoss: 3.103694\n",
      "Train Epoch: 1 [80000/202944 (39%)]\tLoss: 2.997740\n",
      "Train Epoch: 1 [90000/202944 (44%)]\tLoss: 3.134730\n",
      "Train Epoch: 1 [100000/202944 (49%)]\tLoss: 2.999159\n",
      "Train Epoch: 1 [110000/202944 (54%)]\tLoss: 3.186565\n",
      "Train Epoch: 1 [120000/202944 (59%)]\tLoss: 3.074274\n",
      "Train Epoch: 1 [130000/202944 (64%)]\tLoss: 2.971551\n",
      "Train Epoch: 1 [140000/202944 (69%)]\tLoss: 2.972555\n",
      "Train Epoch: 1 [150000/202944 (74%)]\tLoss: 3.071675\n",
      "Train Epoch: 1 [160000/202944 (79%)]\tLoss: 3.320627\n",
      "Train Epoch: 1 [170000/202944 (84%)]\tLoss: 3.102482\n",
      "Train Epoch: 1 [180000/202944 (89%)]\tLoss: 3.041312\n",
      "Train Epoch: 1 [190000/202944 (94%)]\tLoss: 3.043113\n",
      "Train Epoch: 1 [200000/202944 (99%)]\tLoss: 2.936919\n",
      "Epoch: 1 Average loss: 0.0310\n",
      "Train Epoch: 0 [0/202944 (0%)]\tLoss: 4.608556\n",
      "Train Epoch: 0 [10000/202944 (5%)]\tLoss: 3.056599\n",
      "Train Epoch: 0 [20000/202944 (10%)]\tLoss: 3.241711\n",
      "Train Epoch: 0 [30000/202944 (15%)]\tLoss: 2.920060\n",
      "Train Epoch: 0 [40000/202944 (20%)]\tLoss: 3.077103\n",
      "Train Epoch: 0 [50000/202944 (25%)]\tLoss: 3.187973\n",
      "Train Epoch: 0 [60000/202944 (30%)]\tLoss: 3.069382\n",
      "Train Epoch: 0 [70000/202944 (34%)]\tLoss: 3.092443\n",
      "Train Epoch: 0 [80000/202944 (39%)]\tLoss: 3.077458\n",
      "Train Epoch: 0 [90000/202944 (44%)]\tLoss: 3.196841\n",
      "Train Epoch: 0 [100000/202944 (49%)]\tLoss: 3.228688\n",
      "Train Epoch: 0 [110000/202944 (54%)]\tLoss: 3.026193\n",
      "Train Epoch: 0 [120000/202944 (59%)]\tLoss: 3.172092\n",
      "Train Epoch: 0 [130000/202944 (64%)]\tLoss: 3.226108\n",
      "Train Epoch: 0 [140000/202944 (69%)]\tLoss: 3.131218\n",
      "Train Epoch: 0 [150000/202944 (74%)]\tLoss: 3.152434\n",
      "Train Epoch: 0 [160000/202944 (79%)]\tLoss: 3.187236\n",
      "Train Epoch: 0 [170000/202944 (84%)]\tLoss: 3.150728\n",
      "Train Epoch: 0 [180000/202944 (89%)]\tLoss: 3.099062\n",
      "Train Epoch: 0 [190000/202944 (94%)]\tLoss: 3.190163\n",
      "Train Epoch: 0 [200000/202944 (99%)]\tLoss: 3.085424\n",
      "Epoch: 0 Average loss: 0.0316\n",
      "Train Epoch: 1 [0/202944 (0%)]\tLoss: 4.610595\n",
      "Train Epoch: 1 [10000/202944 (5%)]\tLoss: 3.143317\n",
      "Train Epoch: 1 [20000/202944 (10%)]\tLoss: 3.056500\n",
      "Train Epoch: 1 [30000/202944 (15%)]\tLoss: 3.160201\n",
      "Train Epoch: 1 [40000/202944 (20%)]\tLoss: 3.073344\n",
      "Train Epoch: 1 [50000/202944 (25%)]\tLoss: 3.225212\n",
      "Train Epoch: 1 [60000/202944 (30%)]\tLoss: 3.114953\n",
      "Train Epoch: 1 [70000/202944 (34%)]\tLoss: 3.093335\n",
      "Train Epoch: 1 [80000/202944 (39%)]\tLoss: 3.035393\n",
      "Train Epoch: 1 [90000/202944 (44%)]\tLoss: 3.072304\n",
      "Train Epoch: 1 [100000/202944 (49%)]\tLoss: 3.029886\n",
      "Train Epoch: 1 [110000/202944 (54%)]\tLoss: 3.064422\n",
      "Train Epoch: 1 [120000/202944 (59%)]\tLoss: 2.994483\n",
      "Train Epoch: 1 [130000/202944 (64%)]\tLoss: 3.232810\n",
      "Train Epoch: 1 [140000/202944 (69%)]\tLoss: 3.186526\n",
      "Train Epoch: 1 [150000/202944 (74%)]\tLoss: 3.223565\n",
      "Train Epoch: 1 [160000/202944 (79%)]\tLoss: 3.126462\n",
      "Train Epoch: 1 [170000/202944 (84%)]\tLoss: 3.166062\n",
      "Train Epoch: 1 [180000/202944 (89%)]\tLoss: 3.227660\n",
      "Train Epoch: 1 [190000/202944 (94%)]\tLoss: 3.049493\n",
      "Train Epoch: 1 [200000/202944 (99%)]\tLoss: 3.193171\n",
      "Epoch: 1 Average loss: 0.0313\n",
      "Train Epoch: 0 [0/202944 (0%)]\tLoss: 4.665954\n",
      "Train Epoch: 0 [10000/202944 (5%)]\tLoss: 3.177669\n",
      "Train Epoch: 0 [20000/202944 (10%)]\tLoss: 3.189197\n",
      "Train Epoch: 0 [30000/202944 (15%)]\tLoss: 3.177008\n",
      "Train Epoch: 0 [40000/202944 (20%)]\tLoss: 3.059454\n",
      "Train Epoch: 0 [50000/202944 (25%)]\tLoss: 3.316113\n",
      "Train Epoch: 0 [60000/202944 (30%)]\tLoss: 3.415411\n",
      "Train Epoch: 0 [70000/202944 (34%)]\tLoss: 3.160442\n",
      "Train Epoch: 0 [80000/202944 (39%)]\tLoss: 3.305220\n",
      "Train Epoch: 0 [90000/202944 (44%)]\tLoss: 3.104410\n",
      "Train Epoch: 0 [100000/202944 (49%)]\tLoss: 3.089972\n",
      "Train Epoch: 0 [110000/202944 (54%)]\tLoss: 3.115736\n",
      "Train Epoch: 0 [120000/202944 (59%)]\tLoss: 3.257187\n",
      "Train Epoch: 0 [130000/202944 (64%)]\tLoss: 3.141472\n",
      "Train Epoch: 0 [140000/202944 (69%)]\tLoss: 3.309328\n",
      "Train Epoch: 0 [150000/202944 (74%)]\tLoss: 3.219829\n",
      "Train Epoch: 0 [160000/202944 (79%)]\tLoss: 3.353765\n",
      "Train Epoch: 0 [170000/202944 (84%)]\tLoss: 3.304762\n",
      "Train Epoch: 0 [180000/202944 (89%)]\tLoss: 3.147577\n",
      "Train Epoch: 0 [190000/202944 (94%)]\tLoss: 3.341027\n",
      "Train Epoch: 0 [200000/202944 (99%)]\tLoss: 3.144021\n",
      "Epoch: 0 Average loss: 0.0323\n",
      "Train Epoch: 1 [0/202944 (0%)]\tLoss: 4.609214\n",
      "Train Epoch: 1 [10000/202944 (5%)]\tLoss: 3.146984\n",
      "Train Epoch: 1 [20000/202944 (10%)]\tLoss: 3.219743\n",
      "Train Epoch: 1 [30000/202944 (15%)]\tLoss: 3.224859\n",
      "Train Epoch: 1 [40000/202944 (20%)]\tLoss: 3.197780\n",
      "Train Epoch: 1 [50000/202944 (25%)]\tLoss: 3.295126\n",
      "Train Epoch: 1 [60000/202944 (30%)]\tLoss: 3.157899\n",
      "Train Epoch: 1 [70000/202944 (34%)]\tLoss: 2.968723\n",
      "Train Epoch: 1 [80000/202944 (39%)]\tLoss: 3.234417\n",
      "Train Epoch: 1 [90000/202944 (44%)]\tLoss: 3.057713\n",
      "Train Epoch: 1 [100000/202944 (49%)]\tLoss: 3.210947\n",
      "Train Epoch: 1 [110000/202944 (54%)]\tLoss: 3.242844\n",
      "Train Epoch: 1 [120000/202944 (59%)]\tLoss: 3.066718\n",
      "Train Epoch: 1 [130000/202944 (64%)]\tLoss: 3.366566\n",
      "Train Epoch: 1 [140000/202944 (69%)]\tLoss: 3.146933\n",
      "Train Epoch: 1 [150000/202944 (74%)]\tLoss: 3.071389\n",
      "Train Epoch: 1 [160000/202944 (79%)]\tLoss: 3.177642\n",
      "Train Epoch: 1 [170000/202944 (84%)]\tLoss: 3.336883\n",
      "Train Epoch: 1 [180000/202944 (89%)]\tLoss: 3.093018\n",
      "Train Epoch: 1 [190000/202944 (94%)]\tLoss: 3.469923\n",
      "Train Epoch: 1 [200000/202944 (99%)]\tLoss: 3.081903\n",
      "Epoch: 1 Average loss: 0.0324\n",
      "Train Epoch: 0 [0/202944 (0%)]\tLoss: 4.581642\n",
      "Train Epoch: 0 [10000/202944 (5%)]\tLoss: 3.066529\n",
      "Train Epoch: 0 [20000/202944 (10%)]\tLoss: 3.199549\n",
      "Train Epoch: 0 [30000/202944 (15%)]\tLoss: 3.300099\n",
      "Train Epoch: 0 [40000/202944 (20%)]\tLoss: 3.205630\n",
      "Train Epoch: 0 [50000/202944 (25%)]\tLoss: 3.301722\n",
      "Train Epoch: 0 [60000/202944 (30%)]\tLoss: 3.113659\n",
      "Train Epoch: 0 [70000/202944 (34%)]\tLoss: 3.343074\n",
      "Train Epoch: 0 [80000/202944 (39%)]\tLoss: 3.110389\n",
      "Train Epoch: 0 [90000/202944 (44%)]\tLoss: 3.252347\n",
      "Train Epoch: 0 [100000/202944 (49%)]\tLoss: 3.463389\n",
      "Train Epoch: 0 [110000/202944 (54%)]\tLoss: 3.213972\n",
      "Train Epoch: 0 [120000/202944 (59%)]\tLoss: 3.017953\n",
      "Train Epoch: 0 [130000/202944 (64%)]\tLoss: 3.147392\n",
      "Train Epoch: 0 [140000/202944 (69%)]\tLoss: 3.255258\n",
      "Train Epoch: 0 [150000/202944 (74%)]\tLoss: 3.514787\n",
      "Train Epoch: 0 [160000/202944 (79%)]\tLoss: 3.310192\n",
      "Train Epoch: 0 [170000/202944 (84%)]\tLoss: 3.222919\n",
      "Train Epoch: 0 [180000/202944 (89%)]\tLoss: 3.313796\n",
      "Train Epoch: 0 [190000/202944 (94%)]\tLoss: 3.145238\n",
      "Train Epoch: 0 [200000/202944 (99%)]\tLoss: 3.161835\n",
      "Epoch: 0 Average loss: 0.0361\n",
      "Train Epoch: 1 [0/202944 (0%)]\tLoss: 4.555498\n",
      "Train Epoch: 1 [10000/202944 (5%)]\tLoss: 3.293472\n",
      "Train Epoch: 1 [20000/202944 (10%)]\tLoss: 3.352050\n",
      "Train Epoch: 1 [30000/202944 (15%)]\tLoss: 3.256897\n",
      "Train Epoch: 1 [40000/202944 (20%)]\tLoss: 3.302618\n",
      "Train Epoch: 1 [50000/202944 (25%)]\tLoss: 3.097377\n",
      "Train Epoch: 1 [60000/202944 (30%)]\tLoss: 3.106001\n",
      "Train Epoch: 1 [70000/202944 (34%)]\tLoss: 3.243225\n",
      "Train Epoch: 1 [80000/202944 (39%)]\tLoss: 3.178689\n",
      "Train Epoch: 1 [90000/202944 (44%)]\tLoss: 3.254899\n",
      "Train Epoch: 1 [100000/202944 (49%)]\tLoss: 3.118370\n",
      "Train Epoch: 1 [110000/202944 (54%)]\tLoss: 3.308520\n",
      "Train Epoch: 1 [120000/202944 (59%)]\tLoss: 3.177661\n",
      "Train Epoch: 1 [130000/202944 (64%)]\tLoss: 3.273614\n",
      "Train Epoch: 1 [140000/202944 (69%)]\tLoss: 3.237304\n",
      "Train Epoch: 1 [150000/202944 (74%)]\tLoss: 3.510780\n",
      "Train Epoch: 1 [160000/202944 (79%)]\tLoss: 3.216998\n",
      "Train Epoch: 1 [170000/202944 (84%)]\tLoss: 3.324138\n",
      "Train Epoch: 1 [180000/202944 (89%)]\tLoss: 3.313467\n",
      "Train Epoch: 1 [190000/202944 (94%)]\tLoss: 3.336353\n",
      "Train Epoch: 1 [200000/202944 (99%)]\tLoss: 3.143537\n",
      "Epoch: 1 Average loss: 0.0373\n",
      "best lr: 0.1\n",
      "best rmse: tensor(37.5484)\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.01, 0.05, 0.1, 0.5, 1.0]\n",
    "best_rmse = float('inf')\n",
    "ler=0\n",
    "for lr in learning_rates:\n",
    "    \n",
    "    for epoch in range(2):\n",
    "        embedding_sizes=[(2, 1),(2, 1), (2, 1), (2, 1), (2, 1),(2, 1), (2, 1),(2, 1), (2, 1),(2, 1), (2, 1),(2, 1), (5, 1),(30,1),(30,1),(2, 1),(2, 1)]\n",
    "        input_size_continuous = df_continuous.shape[1]\n",
    "        output_size=100\n",
    "        model=CNN(input_size=len(embedding_sizes), embedding_sizes=embedding_sizes, n_continuous=input_size_continuous, output_size=output_size)\n",
    "        criterion=nn.CrossEntropyLoss()\n",
    "        optimizer=optim.Adam(model.parameters(), lr=lr)\n",
    "        train_losses = train1(epoch, model, optimizer, train_loader, verbose=True)    \n",
    "        model.eval()\n",
    "        rmse=calculate_rmse(model,test_loader)\n",
    "        if rmse<best_rmse:\n",
    "            best_rmse=rmse\n",
    "            ler=lr\n",
    "print('best lr:',ler)\n",
    "print('best rmse:',best_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99be4321-7b66-486f-8b83-451d7e97b99c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
